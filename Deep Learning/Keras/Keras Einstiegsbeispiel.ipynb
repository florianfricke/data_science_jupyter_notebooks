{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Einstiegsbeispiel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split #Daten in Trainings und Testdaten splitten\n",
    "from sklearn.preprocessing import StandardScaler #numerischen Daten auf eine Skala zu skalieren\n",
    "from sklearn.metrics import confusion_matrix #Confusion Matrix erzeugen (um Güte des Modells abschätzen)\n",
    "from sklearn.preprocessing import StandardScaler #Fkt um alle numerischen Daten auf eine Skala zu skalieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential #Model importieren\n",
    "from keras.layers import Dense #Layers importieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datenaufbereitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(768, 9)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('../../Daten/csv/diabetes.csv')\n",
    "dataset.head(3)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTrennen der:\\n    äbhängigen Variablen (Features) --> Spalte 1-8 und der \\n    unabhängigen/predictiv Variablen (Labels) --> Spalte 9 / Outcome (hat Diabetes oder nicht)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Trennen der:\n",
    "    äbhängigen Variablen (Features) --> Spalte 1-8 und der \n",
    "    unabhängigen/predictiv Variablen (Labels) --> Spalte 9 / Outcome (hat Diabetes oder nicht)\n",
    "\"\"\"\n",
    "X = dataset.iloc[:,0:8].values\n",
    "y = dataset.iloc[:,8].values #mit values wird ein Array erstellt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.   , 148.   ,  72.   , ...,  33.6  ,   0.627,  50.   ],\n",
       "       [  1.   ,  85.   ,  66.   , ...,  26.6  ,   0.351,  31.   ],\n",
       "       [  8.   , 183.   ,  64.   , ...,  23.3  ,   0.672,  32.   ],\n",
       "       ...,\n",
       "       [  5.   , 121.   ,  72.   , ...,  26.2  ,   0.245,  30.   ],\n",
       "       [  1.   , 126.   ,  60.   , ...,  30.1  ,   0.349,  47.   ],\n",
       "       [  1.   ,  93.   ,  70.   , ...,  30.4  ,   0.315,  23.   ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
       "       1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n",
       "       0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "       1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "       0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "       1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([(i) for i in y if i==1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufteilung der Daten in Test und Trainingsdaten\n",
    "`stratify = y` bei diesem Attribut werden die Daten je nach Anzahl Elementen in den Klassen aufgeteilt.\n",
    "Dies hat bei unausgeglichener Klassenaufteilung wie beispielsweise 100 positive und 5000 negative Beispieldaten eine Vorteil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(614, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(154, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(768, 9)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, b, shuffle=True, stratify = y)\n",
    "X_train.shape\n",
    "X_test.shape\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Scaling nur für X-Values --> Daten normalisieren\n",
    "Wenn wir Neuronale Netze und Deep Learning basierte Systeme nutzen, dann empfiehlt es sich üblicherweise die Daten zu Standardisieren. \n",
    "\n",
    "Der **StandardScaler** transformiert die Daten so, dass ihre Verteilung einen Mittelwert von 0 und ein Standartabweichung von 1 hat.\n",
    "\n",
    "Vergleichbarkeit der DS schaffen, unterschiedliche Verteilungen der Daten --> diese anpassen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Mögl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Mögl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler().fit(X_train)\n",
    "X_train = sc.transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufbau des Neuronalen Netzes\n",
    "1 Input-Layer, 2 Hidden Layer und 1 Output Layer\n",
    "\n",
    "**Dense** = Fully Connected Layer (jedes Neuron ist mit dem nächsten Neuron verbunden)\n",
    "\n",
    "**Sequential** = Das Sequential model ist ein linearer Stack/Stapel von Layern/ Schichten\n",
    "\n",
    "**Input Shape** = *(Anz. Spalten in der Matrix)* Das Model muss wissen welchen Input Shape es bekommt, deshalb wird dieser im 1. Layer angegeben. -> 3. Möglichkeiten\n",
    "\n",
    "Aktivierungsfunktionen:\n",
    "* Hidden Layer: Rectifier-Funktion (ReLU) -> gibt x oder 0 zurück\n",
    "* Output Layer: Sigmoid-Funktion\n",
    "\n",
    "Verwendung des Adam Optimizer für die Kosten/Loss Funktion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n2. Mögl. \\nmodel = Sequential([\\n    Dense(32, activation='relu', input_shape=(784,)),\\n    Dense(10, activation='softmax'),\\n])\\n\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model erzeugen\n",
    "#Sequenzielles Netz, d.h. Layer für Layer\n",
    "model = Sequential()\n",
    "\"\"\"\n",
    "2. Mögl. \n",
    "model = Sequential([\n",
    "    Dense(32, activation='relu', input_shape=(784,)),\n",
    "    Dense(10, activation='softmax'),\n",
    "])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input layer und 1. Hidden Layer erzeugen\n",
    "model.add(Dense(\n",
    "    units = 6, #6 Neuronen im Hidden Layer\n",
    "    kernel_initializer = \"uniform\",\n",
    "    activation = \"relu\", #Verwendung der ReLU Aktivierungsfkt.\n",
    "    input_dim = 8 #8 Neuronen im Input Layer\n",
    "#     2. Mögl.: input_shape = (8,) Angabe des Shapes der Daten\n",
    "#     3. Mögl.: bzw. batch_input_shape = (8,None) None kann man angeben wenn man die Größe nicht kennt\n",
    "))\n",
    "\n",
    "#2. Hidden Layer erzeugen\n",
    "model.add(Dense(\n",
    "    6, #anz units\n",
    "    kernel_initializer = \"uniform\", #Verteilung die vorliegt angeben, uniform = Gleichverteilung\n",
    "    activation = \"relu\",\n",
    "\n",
    "))\n",
    "\n",
    "#Output Layer\n",
    "model.add(Dense(\n",
    "    1,\n",
    "    kernel_initializer = \"uniform\",\n",
    "    activation = \"sigmoid\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zusammenfassung des Modells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 6)                 54        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 42        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 103\n",
      "Trainable params: 103\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lernprozess konfigurieren/compilieren\n",
    "Bevor man das Modell trainieren kann muss man den Lernprozess konfigurieren mit Hilfe der `compile()` Methode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lossfunctions\n",
    "Für ein binäreres (Output: 0 oder 1) Klassifikationsproblem: `loss='binary_crossentropy'`\n",
    "\n",
    "Für ein Mulit-Klassen Klassifikationsproblem: `loss='categorical_crossentropy'`\n",
    "\n",
    "Für ein mean squared error regression problem -> Regressionsproblem: `loss='mse'` --> dafür brauch man eig. keine metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"binary_crossentropy\", #Fehlerminimierungsfkt./Lossfunction/ --> bekommen am Ende Werte zw. 1 oder 0\n",
    "    optimizer = \"adam\", #Optimierungsfkt\n",
    "#     optimizer=optimizers.RMSprop(lr=0.001) #Lernrate manuell festlegen\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For custom metrics\n",
    "# neben der accuracy wird auch der MW mit ausgegeben\n",
    "import keras.backend as K\n",
    "\n",
    "def mean_pred(y_true, y_pred):\n",
    "    return K.mean(y_pred)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', mean_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model anlernen mit Trainingsdaten\n",
    "Zum trainieren von Modellen gibt es 3 Möglichkeiten\n",
    "* `model.fit()`\n",
    "* `model.fit_generator()`\n",
    "* `model.train_on_batch()` --> training fur nur eine batch-size (epoch = 1) --> Output: [loss, accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `fit()`\n",
    "**Input**: Die Eingabedaten von Keras Modellen müssen Numpy-Arrays sein.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`validation_data = tuple (x_val, y_val)`\n",
    "\n",
    "gibt die Loss Function und die Accuracy nach jeder Epoche für z.B. die Testdaten mit aus\n",
    "\n",
    "Das Model wird mit diesen Daten nicht trainiert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modell trainieren + Zeit für Training messen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    data,\n",
    "    labels,\n",
    "    batch_size = 10,  #Aufteilung in Gruppen mit Größe 10\n",
    "    epochs = 5, # Anz. Trainigsdurchdurchläufe -> 10 Batches pro Epcohe\n",
    "    verbose = 1, #Anzeigen eines Trainingsverlaufes -> Default = 1 --> 0 = kein Verlauf anzeigen\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 614 samples, validate on 154 samples\n",
      "Epoch 1/100\n",
      "614/614 [==============================] - 1s 1ms/step - loss: 0.6849 - acc: 0.6515 - mean_pred: 0.4887 - val_loss: 0.6717 - val_acc: 0.6494 - val_mean_pred: 0.4714\n",
      "Epoch 2/100\n",
      "614/614 [==============================] - 0s 441us/step - loss: 0.6427 - acc: 0.6515 - mean_pred: 0.4452 - val_loss: 0.5963 - val_acc: 0.6494 - val_mean_pred: 0.4111\n",
      "Epoch 3/100\n",
      "614/614 [==============================] - 0s 368us/step - loss: 0.5686 - acc: 0.7231 - mean_pred: 0.3793 - val_loss: 0.5166 - val_acc: 0.8117 - val_mean_pred: 0.3535\n",
      "Epoch 4/100\n",
      "614/614 [==============================] - 0s 415us/step - loss: 0.5185 - acc: 0.7704 - mean_pred: 0.3415 - val_loss: 0.4777 - val_acc: 0.7987 - val_mean_pred: 0.3492\n",
      "Epoch 5/100\n",
      "614/614 [==============================] - 0s 324us/step - loss: 0.4961 - acc: 0.7655 - mean_pred: 0.3386 - val_loss: 0.4529 - val_acc: 0.8117 - val_mean_pred: 0.3477\n",
      "Epoch 6/100\n",
      "614/614 [==============================] - 0s 435us/step - loss: 0.4860 - acc: 0.7687 - mean_pred: 0.3366 - val_loss: 0.4424 - val_acc: 0.8117 - val_mean_pred: 0.3594\n",
      "Epoch 7/100\n",
      "614/614 [==============================] - 0s 397us/step - loss: 0.4816 - acc: 0.7671 - mean_pred: 0.3428 - val_loss: 0.4374 - val_acc: 0.8117 - val_mean_pred: 0.3656\n",
      "Epoch 8/100\n",
      "614/614 [==============================] - 0s 350us/step - loss: 0.4796 - acc: 0.7687 - mean_pred: 0.3428 - val_loss: 0.4324 - val_acc: 0.8052 - val_mean_pred: 0.3686\n",
      "Epoch 9/100\n",
      "614/614 [==============================] - 0s 345us/step - loss: 0.4780 - acc: 0.7704 - mean_pred: 0.3450 - val_loss: 0.4295 - val_acc: 0.8052 - val_mean_pred: 0.3665\n",
      "Epoch 10/100\n",
      "614/614 [==============================] - 0s 365us/step - loss: 0.4763 - acc: 0.7752 - mean_pred: 0.3506 - val_loss: 0.4262 - val_acc: 0.8052 - val_mean_pred: 0.3657\n",
      "Epoch 11/100\n",
      "614/614 [==============================] - 0s 352us/step - loss: 0.4754 - acc: 0.7687 - mean_pred: 0.3496 - val_loss: 0.4243 - val_acc: 0.8052 - val_mean_pred: 0.3679\n",
      "Epoch 12/100\n",
      "614/614 [==============================] - 0s 342us/step - loss: 0.4754 - acc: 0.7736 - mean_pred: 0.3488 - val_loss: 0.4231 - val_acc: 0.8052 - val_mean_pred: 0.3666\n",
      "Epoch 13/100\n",
      "614/614 [==============================] - 0s 368us/step - loss: 0.4744 - acc: 0.7687 - mean_pred: 0.3512 - val_loss: 0.4241 - val_acc: 0.8052 - val_mean_pred: 0.3692\n",
      "Epoch 14/100\n",
      "614/614 [==============================] - 0s 487us/step - loss: 0.4742 - acc: 0.7785 - mean_pred: 0.3498 - val_loss: 0.4230 - val_acc: 0.8117 - val_mean_pred: 0.3684\n",
      "Epoch 15/100\n",
      "614/614 [==============================] - 0s 506us/step - loss: 0.4732 - acc: 0.7720 - mean_pred: 0.3467 - val_loss: 0.4221 - val_acc: 0.8117 - val_mean_pred: 0.3705\n",
      "Epoch 16/100\n",
      "614/614 [==============================] - 0s 410us/step - loss: 0.4727 - acc: 0.7752 - mean_pred: 0.3529 - val_loss: 0.4196 - val_acc: 0.8117 - val_mean_pred: 0.3657\n",
      "Epoch 17/100\n",
      "614/614 [==============================] - 0s 383us/step - loss: 0.4723 - acc: 0.7736 - mean_pred: 0.3478 - val_loss: 0.4199 - val_acc: 0.8117 - val_mean_pred: 0.3645\n",
      "Epoch 18/100\n",
      "614/614 [==============================] - 0s 369us/step - loss: 0.4719 - acc: 0.7785 - mean_pred: 0.3532 - val_loss: 0.4191 - val_acc: 0.8117 - val_mean_pred: 0.3645\n",
      "Epoch 19/100\n",
      "614/614 [==============================] - 0s 418us/step - loss: 0.4715 - acc: 0.7769 - mean_pred: 0.3376 - val_loss: 0.4183 - val_acc: 0.8117 - val_mean_pred: 0.3676\n",
      "Epoch 20/100\n",
      "614/614 [==============================] - 0s 384us/step - loss: 0.4719 - acc: 0.7769 - mean_pred: 0.3533 - val_loss: 0.4192 - val_acc: 0.8117 - val_mean_pred: 0.3690\n",
      "Epoch 21/100\n",
      "614/614 [==============================] - 0s 426us/step - loss: 0.4710 - acc: 0.7785 - mean_pred: 0.3510 - val_loss: 0.4194 - val_acc: 0.8117 - val_mean_pred: 0.3734\n",
      "Epoch 22/100\n",
      "614/614 [==============================] - 0s 353us/step - loss: 0.4709 - acc: 0.7785 - mean_pred: 0.3553 - val_loss: 0.4164 - val_acc: 0.8182 - val_mean_pred: 0.3646\n",
      "Epoch 23/100\n",
      "614/614 [==============================] - 0s 374us/step - loss: 0.4703 - acc: 0.7736 - mean_pred: 0.3483 - val_loss: 0.4171 - val_acc: 0.8182 - val_mean_pred: 0.3649\n",
      "Epoch 24/100\n",
      "614/614 [==============================] - 0s 402us/step - loss: 0.4706 - acc: 0.7736 - mean_pred: 0.3532 - val_loss: 0.4177 - val_acc: 0.8182 - val_mean_pred: 0.3635\n",
      "Epoch 25/100\n",
      "614/614 [==============================] - 0s 425us/step - loss: 0.4703 - acc: 0.7720 - mean_pred: 0.3417 - val_loss: 0.4170 - val_acc: 0.8182 - val_mean_pred: 0.3633\n",
      "Epoch 26/100\n",
      "614/614 [==============================] - 0s 430us/step - loss: 0.4693 - acc: 0.7736 - mean_pred: 0.3556 - val_loss: 0.4166 - val_acc: 0.8117 - val_mean_pred: 0.3688\n",
      "Epoch 27/100\n",
      "614/614 [==============================] - 0s 448us/step - loss: 0.4689 - acc: 0.7720 - mean_pred: 0.3497 - val_loss: 0.4177 - val_acc: 0.8182 - val_mean_pred: 0.3708\n",
      "Epoch 28/100\n",
      "614/614 [==============================] - 0s 402us/step - loss: 0.4693 - acc: 0.7704 - mean_pred: 0.3510 - val_loss: 0.4147 - val_acc: 0.8182 - val_mean_pred: 0.3649\n",
      "Epoch 29/100\n",
      "614/614 [==============================] - 0s 365us/step - loss: 0.4693 - acc: 0.7687 - mean_pred: 0.3514 - val_loss: 0.4149 - val_acc: 0.8182 - val_mean_pred: 0.3587\n",
      "Epoch 30/100\n",
      "614/614 [==============================] - 0s 369us/step - loss: 0.4686 - acc: 0.7752 - mean_pred: 0.3484 - val_loss: 0.4158 - val_acc: 0.8182 - val_mean_pred: 0.3651\n",
      "Epoch 31/100\n",
      "614/614 [==============================] - 0s 407us/step - loss: 0.4678 - acc: 0.7769 - mean_pred: 0.3478 - val_loss: 0.4154 - val_acc: 0.8182 - val_mean_pred: 0.3647\n",
      "Epoch 32/100\n",
      "614/614 [==============================] - 0s 387us/step - loss: 0.4675 - acc: 0.7736 - mean_pred: 0.3467 - val_loss: 0.4166 - val_acc: 0.8182 - val_mean_pred: 0.3676\n",
      "Epoch 33/100\n",
      "614/614 [==============================] - 0s 373us/step - loss: 0.4671 - acc: 0.7687 - mean_pred: 0.3526 - val_loss: 0.4159 - val_acc: 0.8182 - val_mean_pred: 0.3674\n",
      "Epoch 34/100\n",
      "614/614 [==============================] - 0s 379us/step - loss: 0.4667 - acc: 0.7736 - mean_pred: 0.3516 - val_loss: 0.4155 - val_acc: 0.8182 - val_mean_pred: 0.3685\n",
      "Epoch 35/100\n",
      "614/614 [==============================] - 0s 418us/step - loss: 0.4668 - acc: 0.7736 - mean_pred: 0.3457 - val_loss: 0.4163 - val_acc: 0.8117 - val_mean_pred: 0.3683\n",
      "Epoch 36/100\n",
      "614/614 [==============================] - 0s 356us/step - loss: 0.4658 - acc: 0.7704 - mean_pred: 0.3500 - val_loss: 0.4163 - val_acc: 0.8117 - val_mean_pred: 0.3699\n",
      "Epoch 37/100\n",
      "614/614 [==============================] - 0s 397us/step - loss: 0.4665 - acc: 0.7704 - mean_pred: 0.3525 - val_loss: 0.4136 - val_acc: 0.8117 - val_mean_pred: 0.3683\n",
      "Epoch 38/100\n",
      "614/614 [==============================] - 0s 412us/step - loss: 0.4658 - acc: 0.7720 - mean_pred: 0.3492 - val_loss: 0.4140 - val_acc: 0.8117 - val_mean_pred: 0.3651\n",
      "Epoch 39/100\n",
      "614/614 [==============================] - 0s 371us/step - loss: 0.4660 - acc: 0.7736 - mean_pred: 0.3501 - val_loss: 0.4154 - val_acc: 0.8052 - val_mean_pred: 0.3669\n",
      "Epoch 40/100\n",
      "614/614 [==============================] - 0s 387us/step - loss: 0.4657 - acc: 0.7752 - mean_pred: 0.3512 - val_loss: 0.4144 - val_acc: 0.8052 - val_mean_pred: 0.3688\n",
      "Epoch 41/100\n",
      "614/614 [==============================] - 0s 441us/step - loss: 0.4649 - acc: 0.7752 - mean_pred: 0.3527 - val_loss: 0.4136 - val_acc: 0.8052 - val_mean_pred: 0.3677\n",
      "Epoch 42/100\n",
      "614/614 [==============================] - 0s 407us/step - loss: 0.4654 - acc: 0.7752 - mean_pred: 0.3494 - val_loss: 0.4137 - val_acc: 0.8052 - val_mean_pred: 0.3663\n",
      "Epoch 43/100\n",
      "614/614 [==============================] - 0s 392us/step - loss: 0.4645 - acc: 0.7752 - mean_pred: 0.3453 - val_loss: 0.4143 - val_acc: 0.8117 - val_mean_pred: 0.3731\n",
      "Epoch 44/100\n",
      "614/614 [==============================] - 0s 381us/step - loss: 0.4644 - acc: 0.7736 - mean_pred: 0.3532 - val_loss: 0.4136 - val_acc: 0.8052 - val_mean_pred: 0.3681\n",
      "Epoch 45/100\n",
      "614/614 [==============================] - 0s 619us/step - loss: 0.4642 - acc: 0.7752 - mean_pred: 0.3474 - val_loss: 0.4136 - val_acc: 0.8052 - val_mean_pred: 0.3707\n",
      "Epoch 46/100\n",
      "614/614 [==============================] - 0s 477us/step - loss: 0.4647 - acc: 0.7704 - mean_pred: 0.3548 - val_loss: 0.4130 - val_acc: 0.8117 - val_mean_pred: 0.3702\n",
      "Epoch 47/100\n",
      "614/614 [==============================] - 0s 352us/step - loss: 0.4640 - acc: 0.7736 - mean_pred: 0.3578 - val_loss: 0.4102 - val_acc: 0.8052 - val_mean_pred: 0.3644\n",
      "Epoch 48/100\n",
      "614/614 [==============================] - 0s 352us/step - loss: 0.4640 - acc: 0.7752 - mean_pred: 0.3482 - val_loss: 0.4102 - val_acc: 0.8117 - val_mean_pred: 0.3681\n",
      "Epoch 49/100\n",
      "614/614 [==============================] - 0s 441us/step - loss: 0.4635 - acc: 0.7720 - mean_pred: 0.3466 - val_loss: 0.4117 - val_acc: 0.8052 - val_mean_pred: 0.3640\n",
      "Epoch 50/100\n",
      "614/614 [==============================] - 0s 347us/step - loss: 0.4633 - acc: 0.7752 - mean_pred: 0.3561 - val_loss: 0.4126 - val_acc: 0.8052 - val_mean_pred: 0.3693\n",
      "Epoch 51/100\n",
      "614/614 [==============================] - 0s 495us/step - loss: 0.4638 - acc: 0.7736 - mean_pred: 0.3453 - val_loss: 0.4115 - val_acc: 0.8052 - val_mean_pred: 0.3668\n",
      "Epoch 52/100\n",
      "614/614 [==============================] - 0s 400us/step - loss: 0.4635 - acc: 0.7736 - mean_pred: 0.3533 - val_loss: 0.4116 - val_acc: 0.8052 - val_mean_pred: 0.3685\n",
      "Epoch 53/100\n",
      "614/614 [==============================] - 0s 361us/step - loss: 0.4629 - acc: 0.7769 - mean_pred: 0.3497 - val_loss: 0.4111 - val_acc: 0.8052 - val_mean_pred: 0.3637\n",
      "Epoch 54/100\n",
      "614/614 [==============================] - 0s 425us/step - loss: 0.4633 - acc: 0.7752 - mean_pred: 0.3453 - val_loss: 0.4121 - val_acc: 0.8052 - val_mean_pred: 0.3713\n",
      "Epoch 55/100\n",
      "614/614 [==============================] - 0s 343us/step - loss: 0.4632 - acc: 0.7818 - mean_pred: 0.3587 - val_loss: 0.4119 - val_acc: 0.8052 - val_mean_pred: 0.3707\n",
      "Epoch 56/100\n",
      "614/614 [==============================] - 0s 356us/step - loss: 0.4623 - acc: 0.7736 - mean_pred: 0.3558 - val_loss: 0.4106 - val_acc: 0.8052 - val_mean_pred: 0.3593\n",
      "Epoch 57/100\n",
      "614/614 [==============================] - 0s 356us/step - loss: 0.4624 - acc: 0.7801 - mean_pred: 0.3370 - val_loss: 0.4107 - val_acc: 0.8052 - val_mean_pred: 0.3672\n",
      "Epoch 58/100\n",
      "614/614 [==============================] - 0s 371us/step - loss: 0.4624 - acc: 0.7752 - mean_pred: 0.3548 - val_loss: 0.4124 - val_acc: 0.8117 - val_mean_pred: 0.3714\n",
      "Epoch 59/100\n",
      "614/614 [==============================] - 0s 350us/step - loss: 0.4620 - acc: 0.7850 - mean_pred: 0.3515 - val_loss: 0.4104 - val_acc: 0.8052 - val_mean_pred: 0.3649\n",
      "Epoch 60/100\n",
      "614/614 [==============================] - 0s 352us/step - loss: 0.4616 - acc: 0.7785 - mean_pred: 0.3553 - val_loss: 0.4100 - val_acc: 0.8052 - val_mean_pred: 0.3681\n",
      "Epoch 61/100\n",
      "614/614 [==============================] - 0s 360us/step - loss: 0.4618 - acc: 0.7785 - mean_pred: 0.3449 - val_loss: 0.4102 - val_acc: 0.8117 - val_mean_pred: 0.3704\n",
      "Epoch 62/100\n",
      "614/614 [==============================] - 0s 360us/step - loss: 0.4621 - acc: 0.7769 - mean_pred: 0.3543 - val_loss: 0.4102 - val_acc: 0.8052 - val_mean_pred: 0.3662\n",
      "Epoch 63/100\n",
      "614/614 [==============================] - 0s 363us/step - loss: 0.4618 - acc: 0.7785 - mean_pred: 0.3500 - val_loss: 0.4089 - val_acc: 0.8052 - val_mean_pred: 0.3656\n",
      "Epoch 64/100\n",
      "614/614 [==============================] - 0s 348us/step - loss: 0.4614 - acc: 0.7818 - mean_pred: 0.3454 - val_loss: 0.4094 - val_acc: 0.8052 - val_mean_pred: 0.3697\n",
      "Epoch 65/100\n",
      "614/614 [==============================] - 0s 360us/step - loss: 0.4618 - acc: 0.7752 - mean_pred: 0.3542 - val_loss: 0.4090 - val_acc: 0.8052 - val_mean_pred: 0.3663\n",
      "Epoch 66/100\n",
      "614/614 [==============================] - 0s 350us/step - loss: 0.4606 - acc: 0.7801 - mean_pred: 0.3518 - val_loss: 0.4087 - val_acc: 0.8052 - val_mean_pred: 0.3618\n",
      "Epoch 67/100\n",
      "614/614 [==============================] - 0s 358us/step - loss: 0.4610 - acc: 0.7785 - mean_pred: 0.3576 - val_loss: 0.4078 - val_acc: 0.8117 - val_mean_pred: 0.3596\n",
      "Epoch 68/100\n",
      "614/614 [==============================] - 0s 400us/step - loss: 0.4608 - acc: 0.7769 - mean_pred: 0.3420 - val_loss: 0.4077 - val_acc: 0.8052 - val_mean_pred: 0.3635\n",
      "Epoch 69/100\n",
      "614/614 [==============================] - 0s 352us/step - loss: 0.4607 - acc: 0.7818 - mean_pred: 0.3460 - val_loss: 0.4075 - val_acc: 0.8052 - val_mean_pred: 0.3680\n",
      "Epoch 70/100\n",
      "614/614 [==============================] - 0s 358us/step - loss: 0.4601 - acc: 0.7769 - mean_pred: 0.3527 - val_loss: 0.4085 - val_acc: 0.8052 - val_mean_pred: 0.3673\n",
      "Epoch 71/100\n",
      "614/614 [==============================] - 0s 423us/step - loss: 0.4607 - acc: 0.7785 - mean_pred: 0.3506 - val_loss: 0.4085 - val_acc: 0.8052 - val_mean_pred: 0.3689\n",
      "Epoch 72/100\n",
      "614/614 [==============================] - 0s 443us/step - loss: 0.4599 - acc: 0.7834 - mean_pred: 0.3512 - val_loss: 0.4083 - val_acc: 0.8052 - val_mean_pred: 0.3665\n",
      "Epoch 73/100\n",
      "614/614 [==============================] - 0s 392us/step - loss: 0.4598 - acc: 0.7850 - mean_pred: 0.3507 - val_loss: 0.4074 - val_acc: 0.8052 - val_mean_pred: 0.3668\n",
      "Epoch 74/100\n",
      "614/614 [==============================] - 0s 334us/step - loss: 0.4597 - acc: 0.7818 - mean_pred: 0.3505 - val_loss: 0.4077 - val_acc: 0.8052 - val_mean_pred: 0.3624\n",
      "Epoch 75/100\n",
      "614/614 [==============================] - 0s 350us/step - loss: 0.4599 - acc: 0.7850 - mean_pred: 0.3485 - val_loss: 0.4083 - val_acc: 0.8052 - val_mean_pred: 0.3684\n",
      "Epoch 76/100\n",
      "614/614 [==============================] - 0s 360us/step - loss: 0.4597 - acc: 0.7769 - mean_pred: 0.3530 - val_loss: 0.4067 - val_acc: 0.8052 - val_mean_pred: 0.3641\n",
      "Epoch 77/100\n",
      "614/614 [==============================] - 0s 379us/step - loss: 0.4594 - acc: 0.7834 - mean_pred: 0.3461 - val_loss: 0.4076 - val_acc: 0.8052 - val_mean_pred: 0.3685\n",
      "Epoch 78/100\n",
      "614/614 [==============================] - 0s 355us/step - loss: 0.4595 - acc: 0.7834 - mean_pred: 0.3475 - val_loss: 0.4069 - val_acc: 0.8052 - val_mean_pred: 0.3680\n",
      "Epoch 79/100\n",
      "614/614 [==============================] - 0s 343us/step - loss: 0.4596 - acc: 0.7785 - mean_pred: 0.3533 - val_loss: 0.4050 - val_acc: 0.8052 - val_mean_pred: 0.3635\n",
      "Epoch 80/100\n",
      "614/614 [==============================] - 0s 348us/step - loss: 0.4592 - acc: 0.7769 - mean_pred: 0.3539 - val_loss: 0.4058 - val_acc: 0.8052 - val_mean_pred: 0.3624\n",
      "Epoch 81/100\n",
      "614/614 [==============================] - 0s 383us/step - loss: 0.4593 - acc: 0.7736 - mean_pred: 0.3552 - val_loss: 0.4042 - val_acc: 0.8117 - val_mean_pred: 0.3609\n",
      "Epoch 82/100\n",
      "614/614 [==============================] - 0s 365us/step - loss: 0.4585 - acc: 0.7850 - mean_pred: 0.3437 - val_loss: 0.4045 - val_acc: 0.8052 - val_mean_pred: 0.3640\n",
      "Epoch 83/100\n",
      "614/614 [==============================] - 0s 363us/step - loss: 0.4585 - acc: 0.7801 - mean_pred: 0.3464 - val_loss: 0.4044 - val_acc: 0.8052 - val_mean_pred: 0.3643\n",
      "Epoch 84/100\n",
      "614/614 [==============================] - 0s 365us/step - loss: 0.4580 - acc: 0.7769 - mean_pred: 0.3529 - val_loss: 0.4073 - val_acc: 0.8117 - val_mean_pred: 0.3728\n",
      "Epoch 85/100\n",
      "614/614 [==============================] - 0s 392us/step - loss: 0.4587 - acc: 0.7752 - mean_pred: 0.3478 - val_loss: 0.4057 - val_acc: 0.8052 - val_mean_pred: 0.3709\n",
      "Epoch 86/100\n",
      "614/614 [==============================] - 0s 425us/step - loss: 0.4577 - acc: 0.7801 - mean_pred: 0.3465 - val_loss: 0.4047 - val_acc: 0.8117 - val_mean_pred: 0.3716\n",
      "Epoch 87/100\n",
      "614/614 [==============================] - 0s 451us/step - loss: 0.4591 - acc: 0.7752 - mean_pred: 0.3560 - val_loss: 0.4064 - val_acc: 0.8117 - val_mean_pred: 0.3723\n",
      "Epoch 88/100\n",
      "614/614 [==============================] - 0s 435us/step - loss: 0.4583 - acc: 0.7785 - mean_pred: 0.3480 - val_loss: 0.4068 - val_acc: 0.8117 - val_mean_pred: 0.3731\n",
      "Epoch 89/100\n",
      "614/614 [==============================] - 0s 379us/step - loss: 0.4577 - acc: 0.7736 - mean_pred: 0.3523 - val_loss: 0.4040 - val_acc: 0.8052 - val_mean_pred: 0.3637\n",
      "Epoch 90/100\n",
      "614/614 [==============================] - 0s 396us/step - loss: 0.4583 - acc: 0.7720 - mean_pred: 0.3485 - val_loss: 0.4060 - val_acc: 0.8117 - val_mean_pred: 0.3730\n",
      "Epoch 91/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "614/614 [==============================] - 0s 392us/step - loss: 0.4579 - acc: 0.7769 - mean_pred: 0.3521 - val_loss: 0.4046 - val_acc: 0.8052 - val_mean_pred: 0.3640\n",
      "Epoch 92/100\n",
      "614/614 [==============================] - 0s 317us/step - loss: 0.4573 - acc: 0.7769 - mean_pred: 0.3409 - val_loss: 0.4060 - val_acc: 0.8117 - val_mean_pred: 0.3730\n",
      "Epoch 93/100\n",
      "614/614 [==============================] - 0s 327us/step - loss: 0.4582 - acc: 0.7818 - mean_pred: 0.3517 - val_loss: 0.4047 - val_acc: 0.8117 - val_mean_pred: 0.3726\n",
      "Epoch 94/100\n",
      "614/614 [==============================] - 0s 324us/step - loss: 0.4576 - acc: 0.7769 - mean_pred: 0.3532 - val_loss: 0.4026 - val_acc: 0.8052 - val_mean_pred: 0.3599\n",
      "Epoch 95/100\n",
      "614/614 [==============================] - 0s 335us/step - loss: 0.4573 - acc: 0.7769 - mean_pred: 0.3513 - val_loss: 0.4025 - val_acc: 0.8052 - val_mean_pred: 0.3634\n",
      "Epoch 96/100\n",
      "614/614 [==============================] - 0s 501us/step - loss: 0.4571 - acc: 0.7834 - mean_pred: 0.3426 - val_loss: 0.4051 - val_acc: 0.8117 - val_mean_pred: 0.3717\n",
      "Epoch 97/100\n",
      "614/614 [==============================] - 0s 552us/step - loss: 0.4571 - acc: 0.7769 - mean_pred: 0.3569 - val_loss: 0.4004 - val_acc: 0.8117 - val_mean_pred: 0.3614\n",
      "Epoch 98/100\n",
      "614/614 [==============================] - 0s 422us/step - loss: 0.4576 - acc: 0.7818 - mean_pred: 0.3498 - val_loss: 0.4033 - val_acc: 0.8117 - val_mean_pred: 0.3625\n",
      "Epoch 99/100\n",
      "614/614 [==============================] - 0s 448us/step - loss: 0.4566 - acc: 0.7801 - mean_pred: 0.3472 - val_loss: 0.4029 - val_acc: 0.8052 - val_mean_pred: 0.3664\n",
      "Epoch 100/100\n",
      "614/614 [==============================] - 0s 350us/step - loss: 0.4563 - acc: 0.7785 - mean_pred: 0.3475 - val_loss: 0.4016 - val_acc: 0.8117 - val_mean_pred: 0.3673\n"
     ]
    }
   ],
   "source": [
    "start_time = time.clock()\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size = batch_size,\n",
    "    epochs = epochs, #Trainigsdurchdurchläufe\n",
    "    validation_data=(X_test, y_test) \n",
    ")\n",
    "elapsed_time = time.clock() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainingsdurchlaufzeit: 24.00s\n"
     ]
    }
   ],
   "source": [
    "print('Trainingsdurchlaufzeit: {0:.2f}s'.format(round(elapsed_time,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modell abspeichern\n",
    "https://www.youtube.com/watch?v=7n1SpeudvAE\n",
    "\n",
    "Die `save()` funktion speichert: \n",
    "* die Architketur des Models und erlaubt dieses wieder zu kreieren\n",
    "* Gewichte des Models\n",
    "* Trainingskonfiguration (optimizer, loss)\n",
    "* den Status des Optimizer, das ermöglicht das Training fort zu setzen, wo man aufgehört hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./Models/\"\n",
    "model_file_name = \"Spelling_Correction_Model\"\n",
    "files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "count = 0\n",
    "for f in files:\n",
    "    if (f.find(model_file_name) != -1):\n",
    "        count +=1\n",
    "filename = '' + path + model_file_name + str(count+1) + '.h5'\n",
    "model.save(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modell laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "new_model = load_model('Spelling_Correction_Model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_model.summary()\n",
    "new_model.get_weights()\n",
    "new_model.optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `fit_generator()`\n",
    "auslagern des Datenladen -> Daten werden nicht komplett in den HS geladen -> für große Datenmengen\n",
    "\n",
    "**Input:** nimmt als Input einen Generator statt ein numpy array\n",
    "\n",
    "*Hinweise:*\n",
    "* Labels und Features müssen beide unbedingt als numpy array zurück gegeben werden\n",
    "* for-Schleifen in der while true Schleife sind nicht optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Genarator**\n",
    "* ein Genrator ist eine unendliche Schleife (deshab while(true)) der immer wieder neue Objekte zurück gibt \n",
    "* Output: Liefert ein Tupel (Eingaben, Ziele) bzw. (inputs, labels)\n",
    "* Alle Arrays sollten die gleiche Anzahl an Samples enthalten.\n",
    "* Es wird erwartet, dass der Generator seine Daten unbegrenzt wiederholt.\n",
    "* Ein Epoch endet, wenn samples_per_epoch Samples vom Modell gesehen wurden\n",
    "\n",
    "[Was ist yield?](https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do/)\n",
    "\n",
    "[Generatoren](https://www.python-kurs.eu/generatoren.php)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator Beispiel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(number = 2):\n",
    "    while True:\n",
    "        if(number%2 == 0): #ist Zahl eine gerade Zahl\n",
    "            yield number #falls zahl gerade, wird diese zurück gegeben\n",
    "        number += 1\n",
    "bsp_generator = generator() #Generator Obj."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "14\n",
      "16\n",
      "18\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(bsp_generator.send(None)) #send: falls man Werte an den Generator senden will (z.B. zahlen als Var.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ablauf**\n",
    "* Der Generator wir aufgerufen mit z.B. 2\n",
    "* Sprung in die while Schleife\n",
    "* if Bed. erfüllt\n",
    "* Rückgabe der Zahl -> Sprung aus Fkt.\n",
    "* bei erneuter Auruf des Generator befinden wir uns weiterhin in der while Schleife\n",
    "* und man startet nun bei den Anweisungen nach yield, also im Bsp. bei number += 1\n",
    "* die Generator merkt sich an welcher Stelle er sich befindet -> d.h. die number wird bei einem erneuten Fkt. Aufruf nicht 2 (da man in der while-Schleife weitermacht) sondern hat den schon iterierten Wert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(features, labels, batch_size):\n",
    "    print('generator initiated')\n",
    "    idx = 0\n",
    "    while True:\n",
    "        i = idx * batch_size\n",
    "        j = min((idx + 1) * batch_size, len(features))\n",
    "        yield features[i:j], labels[i:j]\n",
    "        print('generator yielded a batch %d' % idx)\n",
    "        idx += 1\n",
    "        if j == len(features):\n",
    "            idx = 0\n",
    "tr_gen = gen(X_train, y_train, 10)\n",
    "#break wenn alle Feature gelesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generator initiated\n",
      "generator yielded a batch 0\n",
      "generator yielded a batch 1\n",
      "generator yielded a batch 2\n",
      "generator yielded a batch 3\n",
      "generator yielded a batch 4\n",
      "generator yielded a batch 5\n",
      "generator yielded a batch 6\n",
      "generator yielded a batch 7\n",
      "generator yielded a batch 8\n",
      "Epoch 1/1\n",
      "  1/123 [..............................] - ETA: 0s - loss: 1.6668 - acc: 0.9000generator yielded a batch 9\n",
      "generator yielded a batch 10\n",
      "generator yielded a batch 11\n",
      "generator yielded a batch 12\n",
      "generator yielded a batch 13\n",
      "generator yielded a batch 14\n",
      "generator yielded a batch 15\n",
      "generator yielded a batch 16\n",
      "generator yielded a batch 17\n",
      "generator yielded a batch 18\n",
      "generator yielded a batch 19\n",
      "generator yielded a batch 20\n",
      "generator yielded a batch 21\n",
      "generator yielded a batch 22\n",
      "generator yielded a batch 23\n",
      "generator yielded a batch 24\n",
      "generator yielded a batch 25\n",
      "generator yielded a batch 26\n",
      "generator yielded a batch 27\n",
      "generator yielded a batch 28\n",
      "generator yielded a batch 29\n",
      "generator yielded a batch 30\n",
      "generator yielded a batch 31\n",
      "generator yielded a batch 32\n",
      "generator yielded a batch 33\n",
      "generator yielded a batch 34\n",
      "generator yielded a batch 35\n",
      "generator yielded a batch 36\n",
      "generator yielded a batch 37\n",
      "generator yielded a batch 38\n",
      " 30/123 [======>.......................] - ETA: 0s - loss: 3.7844 - acc: 0.7067generator yielded a batch 39\n",
      "generator yielded a batch 40\n",
      "generator yielded a batch 41\n",
      "generator yielded a batch 42\n",
      "generator yielded a batch 43\n",
      "generator yielded a batch 44\n",
      "generator yielded a batch 45\n",
      "generator yielded a batch 46\n",
      "generator yielded a batch 47\n",
      "generator yielded a batch 48\n",
      "generator yielded a batch 49\n",
      "generator yielded a batch 50\n",
      "generator yielded a batch 51\n",
      "generator yielded a batch 52\n",
      "generator yielded a batch 53\n",
      "generator yielded a batch 54\n",
      "generator yielded a batch 55\n",
      "generator yielded a batch 56\n",
      "generator yielded a batch 57\n",
      "generator yielded a batch 58\n",
      "generator yielded a batch 59\n",
      "generator yielded a batch 60\n",
      " 57/123 [============>.................] - ETA: 0s - loss: 4.4463 - acc: 0.6667generator yielded a batch 61\n",
      "generator yielded a batch 0\n",
      "generator yielded a batch 1\n",
      "generator yielded a batch 2\n",
      "generator yielded a batch 3\n",
      "generator yielded a batch 4\n",
      "generator yielded a batch 5\n",
      "generator yielded a batch 6\n",
      "generator yielded a batch 7\n",
      "generator yielded a batch 8\n",
      "generator yielded a batch 9\n",
      "generator yielded a batch 10\n",
      "generator yielded a batch 11\n",
      "generator yielded a batch 12\n",
      "generator yielded a batch 13\n",
      "generator yielded a batch 14\n",
      "generator yielded a batch 15\n",
      "generator yielded a batch 16\n",
      "generator yielded a batch 17\n",
      "generator yielded a batch 18\n",
      "generator yielded a batch 19\n",
      "generator yielded a batch 20\n",
      "generator yielded a batch 21\n",
      "generator yielded a batch 22\n",
      "generator yielded a batch 23\n",
      "generator yielded a batch 24\n",
      "generator yielded a batch 25\n",
      "generator yielded a batch 26\n",
      "generator yielded a batch 27\n",
      "generator yielded a batch 28\n",
      "generator yielded a batch 29\n",
      " 84/123 [===================>..........] - ETA: 0s - loss: 4.2615 - acc: 0.6756generator yielded a batch 30\n",
      "generator yielded a batch 31\n",
      "generator yielded a batch 32\n",
      "generator yielded a batch 33\n",
      "generator yielded a batch 34\n",
      "generator yielded a batch 35\n",
      "generator yielded a batch 36\n",
      "generator yielded a batch 37\n",
      "generator yielded a batch 38\n",
      "generator yielded a batch 39\n",
      "generator yielded a batch 40\n",
      "generator yielded a batch 41\n",
      "generator yielded a batch 42\n",
      "generator yielded a batch 43\n",
      "generator yielded a batch 44\n",
      "generator yielded a batch 45\n",
      "generator yielded a batch 46\n",
      "generator yielded a batch 47\n",
      "generator yielded a batch 48\n",
      "generator yielded a batch 49\n",
      "generator yielded a batch 50\n",
      "generator yielded a batch 51\n",
      "generator yielded a batch 52\n",
      "generator yielded a batch 53\n",
      "generator yielded a batch 54\n",
      "generator yielded a batch 55\n",
      "generator yielded a batch 56\n",
      "generator yielded a batch 57\n",
      "116/123 [===========================>..] - ETA: 0s - loss: 4.3541 - acc: 0.6668generator yielded a batch 58\n",
      "generator yielded a batch 59\n",
      "generator yielded a batch 60\n",
      "generator yielded a batch 61\n",
      "generator yielded a batch 0\n",
      "generator yielded a batch 1\n",
      "generator yielded a batch 2\n",
      "124/123 [==============================] - 0s 2ms/step - loss: 4.3755 - acc: 0.6645\n",
      "generator yielded a batch 3\n",
      "generator yielded a batch 4\n",
      "generator yielded a batch 5\n",
      "generator yielded a batch 6\n",
      "generator yielded a batch 7\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    generator=tr_gen,\n",
    "    steps_per_epoch=(len(X_train)/batch_size)+1,  #(len(X_train)/batch_size)+1 anzahl der Schritte pro Epoche\n",
    "    max_queue_size=10, #default = 10\n",
    "    workers=6, #default = 1\n",
    "    use_multiprocessing=False #default = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modellvalidierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Güte des Modells mit Testdaten: 78%\n",
    "Prüfen ob Vorhersagen mit den Labels/Predictive Variablen übereinstimmt in dem man die Testdaten auf das Modell anwendet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.84375]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history['acc'][0:4] #Ausgabe der accracy des Trainings -> history.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x23bea662780>]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Accuracy')"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x23bea662cf8>]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Loss Function')"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x23bea66c208>]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Accuracy')"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x23bea66c518>]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Loss Function')"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAGBVJREFUeJzt3X+0XWV95/H3x0SCCEpCggIBgj9mxuAojLdQZ7RSRUDXGCg4DtQf+IM6XdW1OrVMxcGpCrUoarWOrNUy1Bm0KoIsu+hUx2JKZulyUG4kKJHGhCgQghgNoIiC4Hf+ODvtyc1N7knuPfc8uXm/1trr7v08z97nu5/clU/2j9ybqkKSpNY8btQFSJI0GQNKktQkA0qS1CQDSpLUJANKktQkA0qS1CQDSpLUJANKmkKSVUnuS7Jg1LVI+xIDStqFJMuAFwIFrJjFz50/W58ltcqAknbtdcCNwP8Czt3WmOQJST6U5I4kDyT5apIndH0vSPK1JPcnuSvJ67v2VUnO6zvG65N8tW+7krwlyXpgfdf2590xfpJkdZIX9o2fl+S/Jrk9yU+7/iOTXJbkQ/0nkeRvk/znYUyQNCwGlLRrrwM+1S2nJnlK1/5B4HnAvwUWAX8E/CrJUcAXgf8OLAGOA9bsxuedAZwILO+2b+qOsQj4NHBNkv27vrcB5wAvB54EvBF4CLgSOCfJ4wCSLAZeAnxmd05cGjUDStqJJC8AjgaurqrVwO3Ab3d/8b8R+P2quruqHquqr1XVw8CrgS9X1Weq6pdV9eOq2p2AuqSqtlbVzwGq6q+7YzxaVR8CFgD/sht7HvDOqlpXPbd0Y78BPEAvlADOBlZV1b3TnBJpVhlQ0s6dC/x9Vf2o2/5017YY2J9eYE105E7aB3VX/0aSP0xyW3cb8X7gyd3nT/VZVwKv6dZfA3xyGjVJI+GDWGkS3fOkVwHzkvyga14AHAwcBvwCeDpwy4Rd7wJO2MlhfwYc0Lf91EnG/NOvF+ieN72d3pXQ2qr6VZL7gPR91tOBWyc5zl8DtyZ5LvAs4G92UpPULK+gpMmdATxG71nQcd3yLOAr9J5LfRz4sySHdy8rPL97Df1TwMlJXpVkfpJDkhzXHXMNcGaSA5I8A3jTFDUcBDwKbAHmJ/ljes+atrkCuDjJM9PznCSHAFTVJnrPrz4JXLvtlqG0NzGgpMmdC/zPqrqzqn6wbQE+Ru850wXAt+mFwFbg/cDjqupOei8t/GHXvgZ4bnfMDwOPAPfSuwX3qSlq+BK9Fy6+C9xB76qt/xbgnwFXA38P/AT4K+AJff1XAv8ab+9pLxV/YaE0NyX5DXq3+pZV1a9GXY+0u7yCkuagJI8Hfh+4wnDS3sqAkuaYJM8C7qf3MsdHRlyOtMe8xSdJapJXUJKkJjX3/6AWL15cy5YtG3UZkqQhWb169Y+qaslU45oLqGXLljE+Pj7qMiRJQ5LkjkHGeYtPktQkA0qS1CQDSpLUJANKktQkA0qS1CQDSpLUJANKktQkA0qS1CQDSpLUJANKktQkA0qS1CQDSpLUJANKktQkA0qS1KSBAirJaUnWJdmQ5IJJ+o9OsjLJt5KsSrK0r++xJGu65bqZLF6SNHdN+fugkswDLgNeCmwCbkpyXVV9p2/YB4FPVNWVSV4MXAK8tuv7eVUdN8N1S5LmuEGuoE4ANlTVxqp6BLgKOH3CmOXAym79hkn6JUnaLYME1BHAXX3bm7q2frcAZ3XrvwUclOSQbnv/JONJbkxyxrSqlSTtMwYJqEzSVhO2zwdelORm4EXA3cCjXd9RVTUG/DbwkSRP3+EDkjd3ITa+ZcuWwauXJM1ZgwTUJuDIvu2lwOb+AVW1uarOrKrjgQu7tge29XVfNwKrgOMnfkBVXV5VY1U1tmTJkj05D0nSHDNIQN0EPDPJMUn2A84GtnsbL8niJNuO9Q7g4137wiQLto0B/h3Q/3KFJEmTmjKgqupR4K3Al4DbgKuram2Si5Ks6IadBKxL8l3gKcB7u/ZnAeNJbqH38sT7Jrz9J0nSpFI18XHSaI2NjdX4+Pioy5AkDUmS1d27CbvkT5KQJDXJgJIkNcmAkiQ1yYCSJDXJgJIkNcmAkiQ1yYCSJDXJgJIkNcmAkiQ1yYCSJDXJgJIkNcmAkiQ1yYCSJDXJgJIkNcmAkiQ1yYCSJDXJgJIkNcmAkiQ1yYCSJDXJgJIkNcmAkiQ1yYCSJDXJgJIkNcmAkiQ1yYCSJDXJgJIkNcmAkiQ1yYCSJDXJgJIkNcmAkiQ1yYCSJDXJgJIkNcmAkiQ1yYCSJDXJgJIkNcmAkiQ1yYCSJDXJgJIkNWmggEpyWpJ1STYkuWCS/qOTrEzyrSSrkizt6zs3yfpuOXcmi5ckzV1TBlSSecBlwMuA5cA5SZZPGPZB4BNV9RzgIuCSbt9FwLuAE4ETgHclWThz5UuS5qpBrqBOADZU1caqegS4Cjh9wpjlwMpu/Ya+/lOB66tqa1XdB1wPnDb9siVJc90gAXUEcFff9qaurd8twFnd+m8BByU5ZMB9JUnawSABlUnaasL2+cCLktwMvAi4G3h0wH1J8uYk40nGt2zZMkBJkqS5bpCA2gQc2be9FNjcP6CqNlfVmVV1PHBh1/bAIPt2Yy+vqrGqGluyZMlunoIkaS4aJKBuAp6Z5Jgk+wFnA9f1D0iyOMm2Y70D+Hi3/iXglCQLu5cjTunaJEnapSkDqqoeBd5KL1huA66uqrVJLkqyoht2ErAuyXeBpwDv7fbdClxML+RuAi7q2iRJ2qVU7fBIaKTGxsZqfHx81GVIkoYkyeqqGptqnD9JQpLUJANKktQkA0qS1CQDSpLUJANKktQkA0qS1CQDSpLUJANKktQkA0qS1CQDSpLUJANKktQkA0qS1CQDSpLUJANKktQkA0qS1CQDSpLUJANKktQkA0qS1CQDSpLUJANKktQkA0qS1CQDSpLUJANKktSkVNWoa9hOki3AHaOuYwYtBn406iIa45xsz/nYkXOyo7k0J0dX1ZKpBjUXUHNNkvGqGht1HS1xTrbnfOzIOdnRvjgn3uKTJDXJgJIkNcmAGr7LR11Ag5yT7TkfO3JOdrTPzYnPoCRJTfIKSpLUJANKktQkA2oGJFmU5Pok67uvC3cy7txuzPok507Sf12SW4df8XBNZz6SHJDk75L8Y5K1Sd43u9XPrCSnJVmXZEOSCybpX5Dks13/15Ms6+t7R9e+Lsmps1n3MO3pnCR5aZLVSb7dfX3xbNc+DNP5Hun6j0ryYJLzZ6vmWVNVLtNcgEuBC7r1C4D3TzJmEbCx+7qwW1/Y138m8Gng1lGfzyjnAzgA+M1uzH7AV4CXjfqc9nAe5gG3A0/rzuUWYPmEMb8H/EW3fjbw2W59eTd+AXBMd5x5oz6nEc/J8cDh3fqzgbtHfT6jnI++/muBa4DzR30+M714BTUzTgeu7NavBM6YZMypwPVVtbWq7gOuB04DSHIg8DbgT2ah1tmwx/NRVQ9V1Q0AVfUI8E1g6SzUPAwnABuqamN3LlfRm5t+/XP1OeAlSdK1X1VVD1fV94AN3fH2dns8J1V1c1Vt7trXAvsnWTArVQ/PdL5HSHIGvX/crZ2lemeVATUznlJV9wB0Xw+dZMwRwF1925u6NoCLgQ8BDw2zyFk03fkAIMnBwCuAlUOqc9imPMf+MVX1KPAAcMiA++6NpjMn/c4Cbq6qh4dU52zZ4/lI8kTg7cB7ZqHOkZg/6gL2Fkm+DDx1kq4LBz3EJG2V5DjgGVX1BxPvLbdsWPPRd/z5wGeAj1bVxt2vsAm7PMcpxgyy795oOnPS60yOBd4PnDKDdY3KdObjPcCHq+rB7oJqzjGgBlRVJ++sL8m9SQ6rqnuSHAb8cJJhm4CT+raXAquA5wPPS/J9en8ehyZZVVUn0bAhzsc2lwPrq+ojM1DuqGwCjuzbXgps3smYTV0oPxnYOuC+e6PpzAlJlgKfB15XVbcPv9yhm858nAi8MsmlwMHAr5L8oqo+NvyyZ8moH4LNhQX4ANu/FHDpJGMWAd+j9yLAwm590YQxy5gbL0lMaz7oPYu7FnjcqM9lmvMwn97zgWP45wfgx04Y8xa2fwB+dbd+LNu/JLGRufGSxHTm5OBu/FmjPo8W5mPCmHczB1+SGHkBc2Ghd398JbC++7rtL9ox4Iq+cW+k97B7A/CGSY4zVwJqj+eD3r8gC7gNWNMt5436nKYxFy8HvkvvTa0Lu7aLgBXd+v703sDaAHwDeFrfvhd2+61jL32TcSbnBHgn8LO+74s1wKGjPp9Rfo/0HWNOBpQ/6kiS1CTf4pMkNcmAkiQ1yYCSJDXJgJIkNcmAkiQ1yYCSJDXJgJIkNcmAkiQ1yYCSJDXJgJIkNcmAkiQ1yYCSJDXJgJLmsCRHJXkwybxR1yLtLgNKc16S7yfZ6S9YHOLnvj7JY11AbFuG+svkJp5rVd1ZVQdW1WPD/FxpGPyNutJw/b+qesGoi5D2Rl5BaZ+W5HeSbEiyNcl1SQ7v2pPkw0l+mOSBJN9K8uyu7+VJvpPkp0nuTnL+HnzuqiTn9W2/PslX+7Yrye8mWZ/kviSXJcmEum/ravhOkn+T5JPAUcDfdldrf5RkWXes+d1+h3fnubU779/pO+a7k1yd5BPdcdcmGduTeZVmggGlfVaSFwOXAK8CDgPuAK7quk8BfgP4F/R+1fh/BH7c9f0V8J+q6iDg2cA/DKnEfw/8GvDcrsZTu7r/A73foPo64EnACuDHVfVa4E7gFd1tvUsnOeZngE3A4cArgT9N8pK+/hX05uBg4DpgqLckpV0xoLQvezXw8ar6ZlU9DLwDeH6SZcAvgYOAfwWkqm6rqnu6/X4JLE/ypKq6r6q+uYvP+PUk9/ctv74b9b2vqu6vqjuBG4DjuvbzgEur6qbq2VBVd0x1sCRHAi8A3l5Vv6iqNcAVwGv7hn21qr7QPbP6JL1wlEbCgNK+7HB6V00AVNWD9K6Sjqiqf6B39XAZcG+Sy5M8qRt6FvBy4I4k/zfJ83fxGTdW1cF9y427Ud8P+tYfAg7s1o8Ebt+N42xzOLC1qn7a13YHcMQuPnP/bbcHpdlmQGlfthk4ettGkicChwB3A1TVR6vqecCx9G71/Zeu/aaqOh04FPgb4Oo9+OyfAQf0bT91N/a9C3j6TvpqF/ttBhYlOaiv7Si685VaY0BpX/H4JPv3LfOBTwNvSHJckgXAnwJfr6rvJ/m1JCcmeTy9MPkF8FiS/ZK8OsmTq+qXwE+APXmFew1wZpIDkjwDeNNu7HsFcH6S53UvczwjybagvRd42mQ7VdVdwNeAS7o5eE73uZ/ag/qloTOgtK/4AvDzvuXdVbUS+G/AtcA99K5Kzu7GPwn4H8B99G6D/Rj4YNf3WuD7SX4C/C7wmj2o58PAI/QC5Up2IySq6hrgvfQC9qf0ruIWdd2XAO/snndN9nbhOcAyeldTnwfeVVXX70H90tClald3BCRJGg2voCRJTTKgJElNMqAkSU0yoCRJTWruP+AtXry4li1bNuoyJElDsnr16h9V1ZKpxjUXUMuWLWN8fHzUZUiShiTJlD+aC7zFJ0lqlAElSWqSASVJapIBJUlqkgElSWqSASVJapIBJUlqkgElSWqSASVJapIBJUlqkgElSWqSASVJapIBJUlqkgElSWqSASVJapIBJUlqkgElSWqSASVJapIBJUlqkgElSWqSASVJapIBJUlqkgElSWqSASVJapIBJUlqkgElSWqSASVJapIBJUlqkgElSWqSASVJapIBJUlqkgElSWqSASVJapIBJUlqkgElSWrSQAGV5LQk65JsSHLBJP1vS/KdJN9KsjLJ0X19/yfJ/Un+90wWLkma26YMqCTzgMuAlwHLgXOSLJ8w7GZgrKqeA3wOuLSv7wPAa2emXEnSvmKQK6gTgA1VtbGqHgGuAk7vH1BVN1TVQ93mjcDSvr6VwE9nqF5J0j5ikIA6Arirb3tT17YzbwK+uDtFJHlzkvEk41u2bNmdXSVJc9QgAZVJ2mrSgclrgDF6t/UGVlWXV9VYVY0tWbJkd3aVJM1R8wcYswk4sm97KbB54qAkJwMXAi+qqodnpjxJ0r5qkCuom4BnJjkmyX7A2cB1/QOSHA/8JbCiqn4482VKkvY1UwZUVT0KvBX4EnAbcHVVrU1yUZIV3bAPAAcC1yRZk+SfAizJV4BrgJck2ZTk1Bk/C0nSnDPILT6q6gvAFya0/XHf+sm72PeFe1ydJGmf5U+SkCQ1yYCSJDXJgJIkNcmAkiQ1yYCSJDXJgJIkNcmAkiQ1yYCSJDXJgJIkNcmAkiQ1yYCSJDXJgJIkNcmAkiQ1yYCSJDXJgJIkNcmAkiQ1yYCSJDXJgJIkNcmAkiQ1yYCSJDXJgJIkNcmAkiQ1yYCSJDXJgJIkNcmAkiQ1yYCSJDXJgJIkNcmAkiQ1yYCSJDXJgJIkNcmAkiQ1yYCSJDVpoIBKclqSdUk2JLlgkv63JflOkm8lWZnk6L6+c5Os75ZzZ7J4SdLcNWVAJZkHXAa8DFgOnJNk+YRhNwNjVfUc4HPApd2+i4B3AScCJwDvSrJw5sqXJM1Vg1xBnQBsqKqNVfUIcBVwev+Aqrqhqh7qNm8ElnbrpwLXV9XWqroPuB44bWZKlyTNZYME1BHAXX3bm7q2nXkT8MXd2TfJm5OMJxnfsmXLACVJkua6QQIqk7TVpAOT1wBjwAd2Z9+quryqxqpqbMmSJQOUJEma6wYJqE3AkX3bS4HNEwclORm4EFhRVQ/vzr6SJE00SEDdBDwzyTFJ9gPOBq7rH5DkeOAv6YXTD/u6vgSckmRh93LEKV2bJEm7NH+qAVX1aJK30guWecDHq2ptkouA8aq6jt4tvQOBa5IA3FlVK6pqa5KL6YUcwEVVtXUoZyJJmlNSNenjpJEZGxur8fHxUZchSRqSJKuramyqcf4kCUlSkwwoSVKTDChJUpMMKElSkwwoSVKTDChJUpMMKElSkwwoSVKTmvuPukm2AHeMuo4ZtBj40aiLaIxzsj3nY0fOyY7m0pwcXVVT/mTw5gJqrkkyPsj/mN6XOCfbcz525JzsaF+cE2/xSZKaZEBJkppkQA3f5aMuoEHOyfacjx05Jzva5+bEZ1CSpCZ5BSVJapIBJUlqkgE1A5IsSnJ9kvXd14U7GXduN2Z9knMn6b8uya3Dr3i4pjMfSQ5I8ndJ/jHJ2iTvm93qZ1aS05KsS7IhyQWT9C9I8tmu/+tJlvX1vaNrX5fk1Nmse5j2dE6SvDTJ6iTf7r6+eLZrH4bpfI90/UcleTDJ+bNV86ypKpdpLsClwAXd+gXA+ycZswjY2H1d2K0v7Os/E/g0cOuoz2eU8wEcAPxmN2Y/4CvAy0Z9Tns4D/OA24GndedyC7B8wpjfA/6iWz8b+Gy3vrwbvwA4pjvOvFGf04jn5Hjg8G792cDdoz6fUc5HX/+1wDXA+aM+n5levIKaGacDV3brVwJnTDLmVOD6qtpaVfcB1wOnASQ5EHgb8CezUOts2OP5qKqHquoGgKp6BPgmsHQWah6GE4ANVbWxO5er6M1Nv/65+hzwkiTp2q+qqoer6nvAhu54e7s9npOqurmqNnfta4H9kyyYlaqHZzrfIyQ5g94/7tbOUr2zyoCaGU+pqnsAuq+HTjLmCOCuvu1NXRvAxcCHgIeGWeQsmu58AJDkYOAVwMoh1TlsU55j/5iqehR4ADhkwH33RtOZk35nATdX1cNDqnO27PF8JHki8HbgPbNQ50jMH3UBe4skXwaeOknXhYMeYpK2SnIc8Iyq+oOJ95ZbNqz56Dv+fOAzwEerauPuV9iEXZ7jFGMG2XdvNJ056XUmxwLvB06ZwbpGZTrz8R7gw1X1YHdBNecYUAOqqpN31pfk3iSHVdU9SQ4DfjjJsE3ASX3bS4FVwPOB5yX5Pr0/j0OTrKqqk2jYEOdjm8uB9VX1kRkod1Q2AUf2bS8FNu9kzKYulJ8MbB1w373RdOaEJEuBzwOvq6rbh1/u0E1nPk4EXpnkUuBg4FdJflFVHxt+2bNk1A/B5sICfIDtXwq4dJIxi4Dv0XsRYGG3vmjCmGXMjZckpjUf9J7FXQs8btTnMs15mE/v+cAx/PMD8GMnjHkL2z8Av7pbP5btX5LYyNx4SWI6c3JwN/6sUZ9HC/MxYcy7mYMvSYy8gLmw0Ls/vhJY333d9hftGHBF37g30nvYvQF4wyTHmSsBtcfzQe9fkAXcBqzplvNGfU7TmIuXA9+l96bWhV3bRcCKbn1/em9gbQC+ATytb98Lu/3WsZe+yTiTcwK8E/hZ3/fFGuDQUZ/PKL9H+o4xJwPKH3UkSWqSb/FJkppkQEmSmmRASZKaZEBJkppkQEmSmmRASZKaZEBJkpr0/wERIhCA8f/hvQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23bea5c1c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "diag, axes = plt.subplots(nrows = 2, ncols = 1)\n",
    "for ax in axes:\n",
    "    axes[0].plot(history.history['acc'], c=\"b\")\n",
    "    axes[0].set_title('Accuracy')\n",
    "    axes[1].plot(history.history['loss'], c=\"g\")\n",
    "    axes[1].set_title('Loss Function')\n",
    "plt.tight_layout() #verhindert das es keine Überlappungen gibt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_accuracy = history.history['acc']\n",
    "test_accuracy = history.history['val_acc']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_accuracy) + 1)\n",
    "\n",
    "# Visualize accuracy history\n",
    "plt.plot(epoch_count, training_accuracy, 'r--')\n",
    "plt.plot(epoch_count, test_accuracy, 'b-')\n",
    "plt.legend(['Training Accuracy', 'Test Accuracy'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vorhersagen/ Prognosen\n",
    "Um die Güte des neuronalen Netzes zu beurteilen, können nun die Ergebnisse anhand des Testdatensatzes evaluiert werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ausgabe des loss value und der accuracy für das Model mit den Testdaten\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5853474672685125, 0.74025974025974028]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Ausgabe des loss value und der accuracy für das Model mit den Testdaten')\n",
    "model.evaluate(X_test, y_test, batch_size=1, verbose=0)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (y_pred > 0.5) #gibt nur 2 Klassen mit 0 und 1 --> alle Werte unter 0,5 = 0 | über 0,5 = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion-Matrix\n",
    "korrekte Vorhersagen befinden sich in der Diagonalen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True]], dtype=bool)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 1, 0, 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[91, 18],\n",
       "       [17, 28]], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Erzeugen der Confusion Matrix, an welcher die tatsächliche Präzesion bestimmt werden kann\n",
    "y_pred[0:10]\n",
    "y_test[0:10]\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks\n",
    "https://www.youtube.com/watch?v=Gl-N3xr5zLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training eher unterbrechen \n",
    "In keras gibt es Callback Funktionen, welche während eines Trainingsprozesses z.B. am Ende einer Epoche aufgerufen werden können.\n",
    "\n",
    "*Beispiel:*\n",
    "\n",
    "Die Kosten/Loss Funktion der Test/Validationdaten soll nach jeder Epoche überwacht werden.\n",
    "\n",
    "Falls die Loss Function sich nach 2 Epochen nicht verbessert, soll das Training unterbrochen werden.\n",
    "\n",
    "`EarlyStopping()` (vor kompletten Trainingsdurchlauf beenden) `patience = 2` (nach 2 Epochen)\n",
    "\n",
    "Wir werden nicht das beste Modell bekommen, aber das Modell zwei Epochen nach dem besten Modell. ??\n",
    "\n",
    "Mit `ModelCheckpoint` speichern wir das Model in eine Datei nach jedem Checkpoint -> sinvoll bei langandauernden Trainings, da diese ausversehen unterbrechen werden konnten\n",
    "`save_best_only=True` nu rad beste Modell wird abgespeichert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# callback soll das Training eher unterbrechen und das beste Modell speichern\n",
    "callbacks = [EarlyStopping(monitor = 'val_loss', patience = 2),\n",
    "             ModelCheckpoint(filepath = 'Models/best_model.h5', monitor = 'val_loss', save_best_only = True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 614 samples, validate on 154 samples\n",
      "Epoch 1/100\n",
      "614/614 [==============================] - 0s 238us/step - loss: 0.4675 - acc: 0.7704 - val_loss: 0.4446 - val_acc: 0.7792\n",
      "Epoch 2/100\n",
      "614/614 [==============================] - 0s 240us/step - loss: 0.4654 - acc: 0.7704 - val_loss: 0.4447 - val_acc: 0.7857\n",
      "Epoch 3/100\n",
      "614/614 [==============================] - 0s 207us/step - loss: 0.4645 - acc: 0.7704 - val_loss: 0.4484 - val_acc: 0.7857\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x202b5571a90>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size = batch_size,\n",
    "    epochs = epochs,\n",
    "    validation_data = (X_test, y_test),\n",
    "    callbacks = callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard\n",
    "in CMD eingeben: `tensorboard --logdir logs`\n",
    "\n",
    "\n",
    "ein link wird bereit gestellt, z.B. http://flo-pc:6006/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "    from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 614 samples, validate on 154 samples\n",
      "Epoch 1/100\n",
      "614/614 [==============================] - 0s 446us/step - loss: 0.4277 - acc: 0.7932 - val_loss: 0.4620 - val_acc: 0.7792\n",
      "Epoch 2/100\n",
      "614/614 [==============================] - 0s 335us/step - loss: 0.4270 - acc: 0.7964 - val_loss: 0.4650 - val_acc: 0.7857\n",
      "Epoch 3/100\n",
      "614/614 [==============================] - 0s 316us/step - loss: 0.4277 - acc: 0.7964 - val_loss: 0.4655 - val_acc: 0.7857\n",
      "Epoch 4/100\n",
      "614/614 [==============================] - 0s 317us/step - loss: 0.4272 - acc: 0.7948 - val_loss: 0.4662 - val_acc: 0.7792\n",
      "Epoch 5/100\n",
      "614/614 [==============================] - 0s 313us/step - loss: 0.4254 - acc: 0.7964 - val_loss: 0.4681 - val_acc: 0.7792\n",
      "Epoch 6/100\n",
      "614/614 [==============================] - 0s 396us/step - loss: 0.4259 - acc: 0.7964 - val_loss: 0.4690 - val_acc: 0.7792\n",
      "Epoch 7/100\n",
      "614/614 [==============================] - 0s 402us/step - loss: 0.4244 - acc: 0.7948 - val_loss: 0.4660 - val_acc: 0.7857\n",
      "Epoch 8/100\n",
      "614/614 [==============================] - 0s 339us/step - loss: 0.4244 - acc: 0.8013 - val_loss: 0.4662 - val_acc: 0.7922\n",
      "Epoch 9/100\n",
      "614/614 [==============================] - 0s 329us/step - loss: 0.4246 - acc: 0.7980 - val_loss: 0.4661 - val_acc: 0.7857\n",
      "Epoch 10/100\n",
      "614/614 [==============================] - 0s 376us/step - loss: 0.4242 - acc: 0.7948 - val_loss: 0.4644 - val_acc: 0.7857\n",
      "Epoch 11/100\n",
      "614/614 [==============================] - 0s 439us/step - loss: 0.4242 - acc: 0.8013 - val_loss: 0.4633 - val_acc: 0.7857\n",
      "Epoch 12/100\n",
      "614/614 [==============================] - 0s 337us/step - loss: 0.4233 - acc: 0.7964 - val_loss: 0.4669 - val_acc: 0.7922\n",
      "Epoch 13/100\n",
      "614/614 [==============================] - 0s 317us/step - loss: 0.4225 - acc: 0.7997 - val_loss: 0.4697 - val_acc: 0.7857\n",
      "Epoch 14/100\n",
      "614/614 [==============================] - 0s 319us/step - loss: 0.4221 - acc: 0.8029 - val_loss: 0.4691 - val_acc: 0.7857\n",
      "Epoch 15/100\n",
      "614/614 [==============================] - 0s 466us/step - loss: 0.4214 - acc: 0.8013 - val_loss: 0.4671 - val_acc: 0.7922\n",
      "Epoch 16/100\n",
      "614/614 [==============================] - 0s 449us/step - loss: 0.4209 - acc: 0.7997 - val_loss: 0.4673 - val_acc: 0.7987\n",
      "Epoch 17/100\n",
      "614/614 [==============================] - 0s 347us/step - loss: 0.4202 - acc: 0.7997 - val_loss: 0.4694 - val_acc: 0.7857\n",
      "Epoch 18/100\n",
      "614/614 [==============================] - 0s 326us/step - loss: 0.4206 - acc: 0.7997 - val_loss: 0.4685 - val_acc: 0.7987\n",
      "Epoch 19/100\n",
      "614/614 [==============================] - 0s 329us/step - loss: 0.4193 - acc: 0.8029 - val_loss: 0.4700 - val_acc: 0.7922\n",
      "Epoch 20/100\n",
      "614/614 [==============================] - 0s 332us/step - loss: 0.4187 - acc: 0.8062 - val_loss: 0.4700 - val_acc: 0.7922\n",
      "Epoch 21/100\n",
      "614/614 [==============================] - 0s 337us/step - loss: 0.4185 - acc: 0.7997 - val_loss: 0.4639 - val_acc: 0.7922\n",
      "Epoch 22/100\n",
      "614/614 [==============================] - 0s 337us/step - loss: 0.4184 - acc: 0.8029 - val_loss: 0.4675 - val_acc: 0.7987\n",
      "Epoch 23/100\n",
      "614/614 [==============================] - 0s 313us/step - loss: 0.4175 - acc: 0.8094 - val_loss: 0.4762 - val_acc: 0.7792\n",
      "Epoch 24/100\n",
      "614/614 [==============================] - 0s 467us/step - loss: 0.4173 - acc: 0.8013 - val_loss: 0.4785 - val_acc: 0.7792\n",
      "Epoch 25/100\n",
      "614/614 [==============================] - 0s 386us/step - loss: 0.4189 - acc: 0.7932 - val_loss: 0.4713 - val_acc: 0.7857\n",
      "Epoch 26/100\n",
      "614/614 [==============================] - 0s 330us/step - loss: 0.4157 - acc: 0.8013 - val_loss: 0.4719 - val_acc: 0.7857\n",
      "Epoch 27/100\n",
      "614/614 [==============================] - 0s 417us/step - loss: 0.4165 - acc: 0.8078 - val_loss: 0.4719 - val_acc: 0.7857\n",
      "Epoch 28/100\n",
      "614/614 [==============================] - 0s 342us/step - loss: 0.4144 - acc: 0.8046 - val_loss: 0.4718 - val_acc: 0.7857\n",
      "Epoch 29/100\n",
      "614/614 [==============================] - 0s 337us/step - loss: 0.4143 - acc: 0.8078 - val_loss: 0.4722 - val_acc: 0.7792\n",
      "Epoch 30/100\n",
      "614/614 [==============================] - 0s 389us/step - loss: 0.4141 - acc: 0.7980 - val_loss: 0.4727 - val_acc: 0.7792\n",
      "Epoch 31/100\n",
      "614/614 [==============================] - 0s 355us/step - loss: 0.4143 - acc: 0.8094 - val_loss: 0.4727 - val_acc: 0.7857\n",
      "Epoch 32/100\n",
      "614/614 [==============================] - 0s 387us/step - loss: 0.4129 - acc: 0.7980 - val_loss: 0.4747 - val_acc: 0.7857\n",
      "Epoch 33/100\n",
      "614/614 [==============================] - 0s 353us/step - loss: 0.4123 - acc: 0.8046 - val_loss: 0.4711 - val_acc: 0.7857\n",
      "Epoch 34/100\n",
      "614/614 [==============================] - 0s 348us/step - loss: 0.4133 - acc: 0.7997 - val_loss: 0.4745 - val_acc: 0.7792\n",
      "Epoch 35/100\n",
      "614/614 [==============================] - 0s 332us/step - loss: 0.4123 - acc: 0.8094 - val_loss: 0.4756 - val_acc: 0.7857\n",
      "Epoch 36/100\n",
      "614/614 [==============================] - 0s 324us/step - loss: 0.4117 - acc: 0.8062 - val_loss: 0.4768 - val_acc: 0.7792\n",
      "Epoch 37/100\n",
      "614/614 [==============================] - 0s 363us/step - loss: 0.4122 - acc: 0.8078 - val_loss: 0.4808 - val_acc: 0.7727\n",
      "Epoch 38/100\n",
      "614/614 [==============================] - 0s 496us/step - loss: 0.4117 - acc: 0.8078 - val_loss: 0.4771 - val_acc: 0.7792\n",
      "Epoch 39/100\n",
      "614/614 [==============================] - 0s 461us/step - loss: 0.4111 - acc: 0.8127 - val_loss: 0.4786 - val_acc: 0.7792\n",
      "Epoch 40/100\n",
      "614/614 [==============================] - 0s 286us/step - loss: 0.4116 - acc: 0.8046 - val_loss: 0.4739 - val_acc: 0.7857\n",
      "Epoch 41/100\n",
      "614/614 [==============================] - 0s 247us/step - loss: 0.4105 - acc: 0.8062 - val_loss: 0.4773 - val_acc: 0.7792\n",
      "Epoch 42/100\n",
      "614/614 [==============================] - 0s 236us/step - loss: 0.4102 - acc: 0.8062 - val_loss: 0.4793 - val_acc: 0.7857\n",
      "Epoch 43/100\n",
      "614/614 [==============================] - 0s 309us/step - loss: 0.4097 - acc: 0.8078 - val_loss: 0.4752 - val_acc: 0.7857\n",
      "Epoch 44/100\n",
      "614/614 [==============================] - 0s 288us/step - loss: 0.4118 - acc: 0.8046 - val_loss: 0.4794 - val_acc: 0.7792\n",
      "Epoch 45/100\n",
      "614/614 [==============================] - 0s 254us/step - loss: 0.4099 - acc: 0.8078 - val_loss: 0.4806 - val_acc: 0.7792\n",
      "Epoch 46/100\n",
      "614/614 [==============================] - 0s 256us/step - loss: 0.4105 - acc: 0.8143 - val_loss: 0.4807 - val_acc: 0.7727\n",
      "Epoch 47/100\n",
      "614/614 [==============================] - 0s 269us/step - loss: 0.4081 - acc: 0.8029 - val_loss: 0.4805 - val_acc: 0.7727\n",
      "Epoch 48/100\n",
      "614/614 [==============================] - 0s 474us/step - loss: 0.4083 - acc: 0.8078 - val_loss: 0.4789 - val_acc: 0.7792\n",
      "Epoch 49/100\n",
      "614/614 [==============================] - 0s 373us/step - loss: 0.4078 - acc: 0.8029 - val_loss: 0.4820 - val_acc: 0.7727\n",
      "Epoch 50/100\n",
      "614/614 [==============================] - 0s 285us/step - loss: 0.4093 - acc: 0.8111 - val_loss: 0.4833 - val_acc: 0.7597\n",
      "Epoch 51/100\n",
      "614/614 [==============================] - 0s 308us/step - loss: 0.4067 - acc: 0.8078 - val_loss: 0.4814 - val_acc: 0.7662\n",
      "Epoch 52/100\n",
      "614/614 [==============================] - 0s 277us/step - loss: 0.4075 - acc: 0.8094 - val_loss: 0.4843 - val_acc: 0.7597\n",
      "Epoch 53/100\n",
      "614/614 [==============================] - 0s 740us/step - loss: 0.4067 - acc: 0.8078 - val_loss: 0.4821 - val_acc: 0.7727\n",
      "Epoch 54/100\n",
      "614/614 [==============================] - 0s 345us/step - loss: 0.4070 - acc: 0.8127 - val_loss: 0.4861 - val_acc: 0.7792\n",
      "Epoch 55/100\n",
      "614/614 [==============================] - 0s 304us/step - loss: 0.4065 - acc: 0.8127 - val_loss: 0.4857 - val_acc: 0.7727\n",
      "Epoch 56/100\n",
      "614/614 [==============================] - 0s 363us/step - loss: 0.4064 - acc: 0.8094 - val_loss: 0.4897 - val_acc: 0.7597\n",
      "Epoch 57/100\n",
      "614/614 [==============================] - 0s 334us/step - loss: 0.4064 - acc: 0.8094 - val_loss: 0.4842 - val_acc: 0.7727\n",
      "Epoch 58/100\n",
      "614/614 [==============================] - 0s 324us/step - loss: 0.4055 - acc: 0.8111 - val_loss: 0.4822 - val_acc: 0.7597\n",
      "Epoch 59/100\n",
      "614/614 [==============================] - 0s 311us/step - loss: 0.4066 - acc: 0.8160 - val_loss: 0.4864 - val_acc: 0.7532\n",
      "Epoch 60/100\n",
      "614/614 [==============================] - 0s 361us/step - loss: 0.4055 - acc: 0.8111 - val_loss: 0.4878 - val_acc: 0.7662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100\n",
      "614/614 [==============================] - 0s 321us/step - loss: 0.4060 - acc: 0.8127 - val_loss: 0.4888 - val_acc: 0.7597\n",
      "Epoch 62/100\n",
      "614/614 [==============================] - 0s 688us/step - loss: 0.4044 - acc: 0.8111 - val_loss: 0.4867 - val_acc: 0.7727\n",
      "Epoch 63/100\n",
      "614/614 [==============================] - 0s 505us/step - loss: 0.4047 - acc: 0.8094 - val_loss: 0.4858 - val_acc: 0.7727\n",
      "Epoch 64/100\n",
      "614/614 [==============================] - 0s 463us/step - loss: 0.4045 - acc: 0.8127 - val_loss: 0.4900 - val_acc: 0.7662\n",
      "Epoch 65/100\n",
      "614/614 [==============================] - 0s 474us/step - loss: 0.4046 - acc: 0.8143 - val_loss: 0.4841 - val_acc: 0.7662\n",
      "Epoch 66/100\n",
      "614/614 [==============================] - 0s 401us/step - loss: 0.4049 - acc: 0.8094 - val_loss: 0.4935 - val_acc: 0.7597\n",
      "Epoch 67/100\n",
      "614/614 [==============================] - 0s 335us/step - loss: 0.4037 - acc: 0.8046 - val_loss: 0.4840 - val_acc: 0.7597\n",
      "Epoch 68/100\n",
      "614/614 [==============================] - 0s 350us/step - loss: 0.4050 - acc: 0.8078 - val_loss: 0.4865 - val_acc: 0.7662\n",
      "Epoch 69/100\n",
      "614/614 [==============================] - 0s 358us/step - loss: 0.4050 - acc: 0.8029 - val_loss: 0.4963 - val_acc: 0.7532\n",
      "Epoch 70/100\n",
      "614/614 [==============================] - 0s 327us/step - loss: 0.4045 - acc: 0.8111 - val_loss: 0.4915 - val_acc: 0.7597\n",
      "Epoch 71/100\n",
      "614/614 [==============================] - 0s 345us/step - loss: 0.4033 - acc: 0.8127 - val_loss: 0.4924 - val_acc: 0.7532\n",
      "Epoch 72/100\n",
      "614/614 [==============================] - 0s 321us/step - loss: 0.4038 - acc: 0.8111 - val_loss: 0.4892 - val_acc: 0.7597\n",
      "Epoch 73/100\n",
      "614/614 [==============================] - 0s 324us/step - loss: 0.4040 - acc: 0.8160 - val_loss: 0.4955 - val_acc: 0.7468\n",
      "Epoch 74/100\n",
      "614/614 [==============================] - 0s 318us/step - loss: 0.4035 - acc: 0.8208 - val_loss: 0.4889 - val_acc: 0.7597\n",
      "Epoch 75/100\n",
      "614/614 [==============================] - 0s 316us/step - loss: 0.4030 - acc: 0.8111 - val_loss: 0.4934 - val_acc: 0.7532\n",
      "Epoch 76/100\n",
      "614/614 [==============================] - 0s 327us/step - loss: 0.4019 - acc: 0.8094 - val_loss: 0.4955 - val_acc: 0.7468\n",
      "Epoch 77/100\n",
      "614/614 [==============================] - 0s 339us/step - loss: 0.4027 - acc: 0.8094 - val_loss: 0.4875 - val_acc: 0.7532\n",
      "Epoch 78/100\n",
      "614/614 [==============================] - 0s 304us/step - loss: 0.4030 - acc: 0.8143 - val_loss: 0.4922 - val_acc: 0.7532\n",
      "Epoch 79/100\n",
      "614/614 [==============================] - 0s 275us/step - loss: 0.4019 - acc: 0.8078 - val_loss: 0.4921 - val_acc: 0.7597\n",
      "Epoch 80/100\n",
      "614/614 [==============================] - 0s 379us/step - loss: 0.4030 - acc: 0.8078 - val_loss: 0.4882 - val_acc: 0.7662\n",
      "Epoch 81/100\n",
      "614/614 [==============================] - 0s 612us/step - loss: 0.4012 - acc: 0.8111 - val_loss: 0.4973 - val_acc: 0.7468\n",
      "Epoch 82/100\n",
      "614/614 [==============================] - 0s 339us/step - loss: 0.4015 - acc: 0.8143 - val_loss: 0.4903 - val_acc: 0.7532\n",
      "Epoch 83/100\n",
      "614/614 [==============================] - 0s 311us/step - loss: 0.4014 - acc: 0.8094 - val_loss: 0.4957 - val_acc: 0.7532\n",
      "Epoch 84/100\n",
      "614/614 [==============================] - 0s 342us/step - loss: 0.4001 - acc: 0.8176 - val_loss: 0.4902 - val_acc: 0.7597\n",
      "Epoch 85/100\n",
      "614/614 [==============================] - 0s 412us/step - loss: 0.4008 - acc: 0.8143 - val_loss: 0.4910 - val_acc: 0.7597\n",
      "Epoch 86/100\n",
      "614/614 [==============================] - 0s 297us/step - loss: 0.4008 - acc: 0.8111 - val_loss: 0.4911 - val_acc: 0.7468\n",
      "Epoch 87/100\n",
      "614/614 [==============================] - 0s 290us/step - loss: 0.4020 - acc: 0.8111 - val_loss: 0.4914 - val_acc: 0.7532\n",
      "Epoch 88/100\n",
      "614/614 [==============================] - 0s 277us/step - loss: 0.4005 - acc: 0.8062 - val_loss: 0.4894 - val_acc: 0.7597\n",
      "Epoch 89/100\n",
      "614/614 [==============================] - 0s 277us/step - loss: 0.4005 - acc: 0.8143 - val_loss: 0.4990 - val_acc: 0.7403\n",
      "Epoch 90/100\n",
      "614/614 [==============================] - 0s 324us/step - loss: 0.4013 - acc: 0.8160 - val_loss: 0.4901 - val_acc: 0.7468\n",
      "Epoch 91/100\n",
      "614/614 [==============================] - 0s 300us/step - loss: 0.3997 - acc: 0.8094 - val_loss: 0.4957 - val_acc: 0.7403\n",
      "Epoch 92/100\n",
      "614/614 [==============================] - 0s 285us/step - loss: 0.3996 - acc: 0.8127 - val_loss: 0.4874 - val_acc: 0.7727\n",
      "Epoch 93/100\n",
      "614/614 [==============================] - 0s 278us/step - loss: 0.4000 - acc: 0.8127 - val_loss: 0.4979 - val_acc: 0.7403\n",
      "Epoch 94/100\n",
      "614/614 [==============================] - 0s 285us/step - loss: 0.3989 - acc: 0.8160 - val_loss: 0.4921 - val_acc: 0.7532\n",
      "Epoch 95/100\n",
      "614/614 [==============================] - 0s 303us/step - loss: 0.3987 - acc: 0.8143 - val_loss: 0.4933 - val_acc: 0.7532\n",
      "Epoch 96/100\n",
      "614/614 [==============================] - 0s 332us/step - loss: 0.3996 - acc: 0.8062 - val_loss: 0.4927 - val_acc: 0.7532\n",
      "Epoch 97/100\n",
      "614/614 [==============================] - 0s 322us/step - loss: 0.4003 - acc: 0.8176 - val_loss: 0.4880 - val_acc: 0.7468\n",
      "Epoch 98/100\n",
      "614/614 [==============================] - 0s 282us/step - loss: 0.3982 - acc: 0.8046 - val_loss: 0.4991 - val_acc: 0.7403\n",
      "Epoch 99/100\n",
      "614/614 [==============================] - 0s 275us/step - loss: 0.3998 - acc: 0.8094 - val_loss: 0.4923 - val_acc: 0.7403\n",
      "Epoch 100/100\n",
      "614/614 [==============================] - 0s 282us/step - loss: 0.3990 - acc: 0.8176 - val_loss: 0.4898 - val_acc: 0.7597\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23a70eb0f60>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size = batch_size,\n",
    "    epochs = epochs,\n",
    "    validation_data = (X_test, y_test),\n",
    "    callbacks = [\n",
    "        TensorBoard(log_dir='./logs',)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
