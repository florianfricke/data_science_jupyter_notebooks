{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification with an Keras Embedding Layer\n",
    "Verwendung eines RNNs in Verbindung mit LSTM\n",
    "\n",
    "TF-IDF = anderes bag of word model encoding\n",
    "\n",
    "**mehr Informationen befinden sich im Machine Learning Dokument (word)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow Embedding: https://www.tensorflow.org/programmers_guide/embedding\n",
    "\n",
    "https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "\n",
    "https://github.com/transcranial/keras-js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "import itertools\n",
    "from numpy import array, asarray, zeros\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Flatten\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beispiel 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 Trainingsdaten, 25000 Testdaten\n"
     ]
    }
   ],
   "source": [
    "vokabel_anz = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vokabel_anz)\n",
    "print('{} Trainingsdaten, {} Testdaten'.format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Filmreview---\n",
      "[1, 2, 365, 1234, 5, 1156, 354, 11, 14, 2, 2, 7, 1016, 2, 2, 356, 44, 4, 1349, 500, 746, 5, 200, 4, 4132, 11, 2, 2, 1117, 1831, 2, 5, 4831, 26, 6, 2, 4183, 17, 369, 37, 215, 1345, 143, 2, 5, 1838, 8, 1974, 15, 36, 119, 257, 85, 52, 486, 9, 6, 2, 2, 63, 271, 6, 196, 96, 949, 4121, 4, 2, 7, 4, 2212, 2436, 819, 63, 47, 77, 2, 180, 6, 227, 11, 94, 2494, 2, 13, 423, 4, 168, 7, 4, 22, 5, 89, 665, 71, 270, 56, 5, 13, 197, 12, 161, 2, 99, 76, 23, 2, 7, 419, 665, 40, 91, 85, 108, 7, 4, 2084, 5, 4773, 81, 55, 52, 1901]\n",
      "---label---\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print('---Filmreview---')\n",
    "print(X_train[6])\n",
    "print('---label---')\n",
    "print(y_train[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---word2id---\n",
      "{'fawn': 34701, 'tsukino': 52006, 'nunnery': 52007, 'sonja': 16816, 'vani': 63951, 'woods': 1408, 'spiders': 16115, 'hanging': 2345, 'woody': 2289, 'trawling': 52008}\n",
      "---id2word---\n",
      "{34701: 'fawn', 52006: 'tsukino', 52007: 'nunnery', 16816: 'sonja', 63951: 'vani', 1408: 'woods', 16115: 'spiders', 2345: 'hanging', 2289: 'woody', 52008: 'trawling'}\n",
      "---review with words---\n",
      "['the', 'and', 'full', 'involving', 'to', 'impressive', 'boring', 'this', 'as', 'and', 'and', 'br', 'villain', 'and', 'and', 'need', 'has', 'of', 'costumes', 'b', 'message', 'to', 'may', 'of', 'props', 'this', 'and', 'and', 'concept', 'issue', 'and', 'to', \"god's\", 'he', 'is', 'and', 'unfolds', 'movie', 'women', 'like', \"isn't\", 'surely', \"i'm\", 'and', 'to', 'toward', 'in', \"here's\", 'for', 'from', 'did', 'having', 'because', 'very', 'quality', 'it', 'is', 'and', 'and', 'really', 'book', 'is', 'both', 'too', 'worked', 'carl', 'of', 'and', 'br', 'of', 'reviewer', 'closer', 'figure', 'really', 'there', 'will', 'and', 'things', 'is', 'far', 'this', 'make', 'mistakes', 'and', 'was', \"couldn't\", 'of', 'few', 'br', 'of', 'you', 'to', \"don't\", 'female', 'than', 'place', 'she', 'to', 'was', 'between', 'that', 'nothing', 'and', 'movies', 'get', 'are', 'and', 'br', 'yes', 'female', 'just', 'its', 'because', 'many', 'br', 'of', 'overly', 'to', 'descent', 'people', 'time', 'very', 'bland']\n",
      "---label---\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Mapping Integers to Words\n",
    "word2id = imdb.get_word_index()\n",
    "print('---word2id---')\n",
    "print(dict(itertools.islice(word2id.items(), 10)))\n",
    "\n",
    "id2word = {i: word for word, i in word2id.items()}\n",
    "print('---id2word---')\n",
    "print(dict(itertools.islice(id2word.items(), 10)))\n",
    "\n",
    "print('---review with words---')\n",
    "print([id2word.get(i, ' ') for i in X_train[6]])\n",
    "print('---label---')\n",
    "print(y_train[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "längstes Review mit {} Zeichen 2697\n",
      "kürzestes Review mit {} Zeichen 70\n"
     ]
    }
   ],
   "source": [
    "print(\"längstes Review mit {} Zeichen\", len(max((X_train + X_test), key=len)))\n",
    "print(\"kürzestes Review mit {} Zeichen\", len(min((X_train + X_test), key=len)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pad Sequences\n",
    "**Bei Daten mit unterschiedlicher Länge an Wörtern pro Satz**\n",
    "\n",
    "Um das RNN mit Daten zu füttern, benötigen die Texte die gleiche Länge. Zu lange Sätze werden abgeschnitten und in kleinere Sätze aufgeteilt. Zu kurze Sätze werden mit 0en aufgefüllt.\n",
    "--> Verwendung der pad_sequences() Fkt. in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "# die Matrix hat nun 500 Spalten --> Reviews unter 500 Wörten werden mit 0 aufgefüllt und Reviews über 500 gesplittet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Layer\n",
    "* wird als erster Hidden-Layer im Netwerk definiert\n",
    "* wird mit zufälligen Gewichten initialisiert und lernt das Embedding für alle Wörter im Trianingsset\n",
    "* d.h. die Gewichte der Wörter werden gelernt\n",
    "* Vorraussetzung: die Wörter der Inputtexte müssen zu Integerzahlen aus dem Vokabular geparst werden\n",
    "* benötigt 3 Argumente:\n",
    "    * **input_dim**: Größe des Vokabulars (z.B. Integerzahlen von 0-100, Vokabular = 101)\n",
    "    * **output_dim**: Größe/Dim des Outputvektors in den die Wörter embedded sind\n",
    "    * **input_length**: Länge der Input Sequenz (z.B. Anz der Inputspalten der Matrix) z.B. 4 durch pad_sequence\n",
    "    * **trainable**: false, Wortvektoren werden während des Trainings nicht angepasst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mask_zero (Argument Embedding Layer)\n",
    "Wenn der Eingabewert 0 ein durch padding (auffüllen) ausgeblendet werden soll. \n",
    "nützlich, wenn wiederkehrende Layer verwendet werden, die Eingaben mit variabler Länge benötigen. \n",
    "Wenn dies der TrueFall ist, müssen alle nachfolgenden Schichten im Modell die Maskierung unterstützen oder es wird eine Ausnahme ausgelöst. \n",
    "Wenn mask_zero auf True gesetzt ist, kann der Index 0 nicht im Vokabular verwendet werden (input_dim sollte die Größe des Vokabulars + 1 haben).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Output des Embedding Layers ist ein 2 Dim. Matrix: mit einem Embedding für jedes Wort in der input Sequenz der Wörter (Input Dokument).\n",
    "Um  nach einem Embedding Layer ein Dense Layer zu verwenden muss man vorher einen Flatten-Layer intigrieren. (2 Dim Matrix zu 1 Dim Vektor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_size=32\n",
    "model=Sequential()\n",
    "model.add(Embedding(vokabel_anz, embedding_size, input_length=max_words))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24936 samples, validate on 64 samples\n",
      "Epoch 1/3\n",
      "24936/24936 [==============================] - 371s 15ms/step - loss: 0.4147 - acc: 0.8039 - val_loss: 0.2270 - val_acc: 0.9219\n",
      "Epoch 2/3\n",
      "24936/24936 [==============================] - 389s 16ms/step - loss: 0.2624 - acc: 0.8960 - val_loss: 0.2198 - val_acc: 0.9062\n",
      "Epoch 3/3\n",
      "24936/24936 [==============================] - 381s 15ms/step - loss: 0.2596 - acc: 0.8973 - val_loss: 0.2736 - val_acc: 0.9062\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29424869fd0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 3\n",
    "\n",
    "X_valid, y_valid = X_train[:batch_size], y_train[:batch_size]\n",
    "X_train2, y_train2 = X_train[batch_size:], y_train[batch_size:]\n",
    "\n",
    "model.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./Models/keras_sentiment_analysis.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "new_model = load_model(\"./Models/keras_sentiment_analysis.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-7218d0701051>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test accuracy:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3_Neu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[0;32m   1114\u001b[0m                                          \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1115\u001b[0m                                          \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1116\u001b[1;33m                                          steps=steps)\n\u001b[0m\u001b[0;32m   1117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1118\u001b[0m     def predict(self, x,\n",
      "\u001b[1;32m~\\Anaconda3_Neu\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m    396\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 398\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    399\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3_Neu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2661\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2662\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2663\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3_Neu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2630\u001b[0m                                 session)\n\u001b[1;32m-> 2631\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2632\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3_Neu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[1;32m-> 1451\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01096611],\n",
       "       [ 0.9954626 ]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#review = [1, 2, 365, 1234, 5, 1156, 354, 11, 14, 2, 2, 7, 1016, 2, 2, 356, 44, 4, 1349, 500, 746, 5, 200, 4, 4132, 11, 2, 2, 1117, 1831, 2, 5, 4831, 26, 6, 2, 4183, 17, 369, 37, 215, 1345, 143, 2, 5, 1838, 8, 1974, 15, 36, 119, 257, 85, 52, 486, 9, 6, 2, 2, 63, 271, 6, 196, 96, 949, 4121, 4, 2, 7, 4, 2212, 2436, 819, 63, 47, 77, 2, 180, 6, 227, 11, 94, 2494, 2, 13, 423, 4, 168, 7, 4, 22, 5, 89, 665, 71, 270, 56, 5, 13, 197, 12, 161, 2, 99, 76, 23, 2, 7, 419, 665, 40, 91, 85, 108, 7, 4, 2084, 5, 4773, 81, 55, 52, 1901]\n",
    "#review_pad = sequence.pad_sequences(review, maxlen=500)\n",
    "model.predict(X_test[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beispiel 2\n",
    "von https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ['Well done!',\n",
    "        'Good work',\n",
    "        'Great effort',\n",
    "        'nice work',\n",
    "        'Excellent!',\n",
    "        'Weak',\n",
    "        'Poor effort!',\n",
    "        'not good',\n",
    "        'poor work',\n",
    "        'Could have done better.']\n",
    "# define class labels\n",
    "labels = array([1,1,1,1,1,0,0,0,0,0])\n",
    "# integer encode the documents\n",
    "vocab_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one hot encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11, 40], [18, 13], [37, 48], [42, 13], [17], [42], [16, 48], [1, 18], [16, 13], [7, 21, 40, 10]]\n"
     ]
    }
   ],
   "source": [
    "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pad-sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11 40  0  0]\n",
      " [18 13  0  0]\n",
      " [37 48  0  0]\n",
      " [42 13  0  0]\n",
      " [17  0  0  0]\n",
      " [42  0  0  0]\n",
      " [16 48  0  0]\n",
      " [ 1 18  0  0]\n",
      " [16 13  0  0]\n",
      " [ 7 21 40 10]]\n"
     ]
    }
   ],
   "source": [
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model + Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 8, input_length=max_length)) \n",
    "#(50, 8, 4) --> 50 Wörter im Vokabular, 8 Spalten Outputmatrix, 4 Spalten Inputmatrix\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 4, 8)              400       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 433\n",
      "Trainable params: 433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.000000\n",
      "Loss: 0.283767\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))\n",
    "print('Loss: %f' % loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beispiel mit vortrainierten Wortvektoren (GloVe Embedding)\n",
    "von https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define documents\n",
    "docs = ['Well done!',\n",
    "        'Good work',\n",
    "        'Great effort',\n",
    "        'nice work',\n",
    "        'Excellent!',\n",
    "        'Weak',\n",
    "        'Poor effort!',\n",
    "        'not good',\n",
    "        'poor work',\n",
    "        'Could have done better.']\n",
    "# define class labels\n",
    "labels = array([1,1,1,1,1,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'work': 1, 'done': 2, 'good': 3, 'effort': 4, 'poor': 5, 'well': 6, 'great': 7, 'nice': 8, 'excellent': 9, 'weak': 10, 'not': 11, 'could': 12, 'have': 13, 'better': 14}\n",
      "Vokabelanzahl: 15\n"
     ]
    }
   ],
   "source": [
    "# prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "print(t.word_index)\n",
    "print(\"Vokabelanzahl: {}\".format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 2], [3, 1], [7, 4], [8, 1], [9], [10], [5, 4], [11, 3], [5, 1], [12, 13, 2, 14]]\n"
     ]
    }
   ],
   "source": [
    "# integer encode the documents\n",
    "# die Beispiele müssen als Ganzzahlen codiert werden u\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6  2  0  0]\n",
      " [ 3  1  0  0]\n",
      " [ 7  4  0  0]\n",
      " [ 8  1  0  0]\n",
      " [ 9  0  0  0]\n",
      " [10  0  0  0]\n",
      " [ 5  4  0  0]\n",
      " [11  3  0  0]\n",
      " [ 5  1  0  0]\n",
      " [12 13  2 14]]\n"
     ]
    }
   ],
   "source": [
    "# pad documents to a max length of 4 words\n",
    "# die Sequenzen müssen auf die gleiche Länge aufgefüllt warden\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_ascii(self, text):\n",
    "        try:\n",
    "            text.encode('ascii')\n",
    "            return True\n",
    "        except:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_dict = dict()\n",
    "with open('../../Daten/glove.6B/glove.6B.50d.txt', \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = asarray(values[1:], dtype='float32')\n",
    "\n",
    "        if word not in embeddings_dict or word.strip() == \"\":\n",
    "            embeddings_dict[word] = coefs\n",
    "print('Loaded %s word vectors.' % len(embeddings_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
       "       -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
       "        2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
       "        1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
       "       -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
       "       -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
       "        4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
       "        7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
       "       -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
       "        1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# das Wort gefolgt mit 100 Zahlen/ Gewichten -> Datei mit 100 oder 50 Dim -> Anzahl Zahlen hinter dem Wort\n",
    "embeddings_dict['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Müssen Wörter in Zahlen und Zahlen in Wörter mappen\n",
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, 50)) #Matrix mit 15 Zeilen und 50 Spalten\n",
    "for word, i in t.word_index.items(): #{'work': 1, 'done': 2,...}\n",
    "    embedding_vector = embeddings_dict.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 5.13589978e-01  1.96950004e-01 -5.19439995e-01 -8.62179995e-01\n",
      "   1.54940002e-02  1.09729998e-01 -8.02929997e-01 -3.33609998e-01\n",
      "  -1.61189993e-04  1.01889996e-02  4.67340015e-02  4.67510015e-01\n",
      "  -4.74750012e-01  1.10380001e-01  3.93269986e-01 -4.36520010e-01\n",
      "   3.99839997e-01  2.71090001e-01  4.26499993e-01 -6.06400013e-01\n",
      "   8.11450005e-01  4.56299990e-01 -1.27260000e-01 -2.24739999e-01\n",
      "   6.40709996e-01 -1.27670002e+00 -7.22310007e-01 -6.95900023e-01\n",
      "   2.80450005e-02 -2.30719998e-01  3.79959989e+00 -1.26249999e-01\n",
      "  -4.79669988e-01 -9.99719977e-01 -2.19760001e-01  5.05649984e-01\n",
      "   2.59530004e-02  8.05140018e-01  1.99290007e-01  2.87959993e-01\n",
      "  -1.59150004e-01 -3.04380000e-01  1.60249993e-01 -1.82899997e-01\n",
      "  -3.85629982e-02 -1.76190004e-01  2.70409994e-02  4.68420014e-02\n",
      "  -6.28970027e-01  3.57259989e-01]\n",
      " [ 3.30760002e-01 -4.38699991e-01 -3.21630001e-01 -4.93099988e-01\n",
      "   1.02540001e-01 -2.74210004e-03 -5.17199993e-01  2.43360009e-02\n",
      "  -1.28160000e-01  1.43490002e-01 -1.66909993e-01  5.61209977e-01\n",
      "  -5.62409997e-01 -4.09720019e-02  7.50000000e-01  2.30839998e-01\n",
      "   5.32040000e-01 -4.09730002e-02  2.68920004e-01 -6.92380011e-01\n",
      "   2.78829992e-01  3.79110008e-01  5.63899994e-01 -3.81500006e-01\n",
      "   7.21319973e-01 -1.35619998e+00 -8.17170024e-01 -5.48419990e-02\n",
      "   5.73329985e-01 -8.54889989e-01  3.18889999e+00  1.99180007e-01\n",
      "  -4.21200007e-01 -9.04269993e-01 -1.95209995e-01  3.01109999e-01\n",
      "   4.67559993e-01  8.21300030e-01  6.05520010e-02 -1.61430001e-01\n",
      "  -2.66680002e-01 -1.76599994e-01  1.58200003e-02  2.55279988e-01\n",
      "  -9.67390016e-02 -9.72819999e-02 -8.44829977e-02  3.33119988e-01\n",
      "  -2.22519994e-01  7.44570017e-01]\n",
      " [-3.55859995e-01  5.21300018e-01 -6.10700011e-01 -3.01310003e-01\n",
      "   9.48620021e-01 -3.15389991e-01 -5.98309994e-01  1.21880002e-01\n",
      "  -3.19430009e-02  5.56949973e-01 -1.06210001e-01  6.33989990e-01\n",
      "  -4.73399997e-01 -7.58949965e-02  3.82470012e-01  8.15690011e-02\n",
      "   8.22139978e-01  2.22200006e-01 -8.37639999e-03 -7.66200006e-01\n",
      "  -5.62529981e-01  6.17590010e-01  2.02920005e-01 -4.85979989e-02\n",
      "   8.78149986e-01 -1.65489995e+00 -7.74179995e-01  1.54349998e-01\n",
      "   9.48230028e-01 -3.95200014e-01  3.73020005e+00  8.28549981e-01\n",
      "  -1.41039997e-01  1.63950007e-02  2.11150005e-01 -3.60849984e-02\n",
      "  -1.55870005e-01  8.65830004e-01  2.63090014e-01 -7.10150003e-01\n",
      "  -3.67700011e-02  1.82819995e-03 -1.77039996e-01  2.70319998e-01\n",
      "   1.10260002e-01  1.41330004e-01 -5.73219992e-02  2.72069991e-01\n",
      "   3.13050002e-01  9.27709997e-01]\n",
      " [ 7.09020019e-01 -6.08609974e-01  2.64470011e-01 -5.45560002e-01\n",
      "  -8.00559968e-02  1.54640004e-01 -5.39529979e-01  3.18520010e-01\n",
      "   7.45949984e-01  4.50019985e-01 -6.79440022e-01 -9.34159979e-02\n",
      "  -6.25569999e-01  1.66010007e-01 -3.48589987e-01 -5.37109971e-01\n",
      "   1.00380003e+00 -5.46079993e-01  1.47909999e-01 -3.78930002e-01\n",
      "   3.24259996e-01  4.77820002e-02 -7.14290023e-01 -1.00329995e+00\n",
      "   2.08430007e-01 -1.77559996e+00  1.24679998e-01 -6.15610003e-01\n",
      "   4.94029999e-01 -6.83329999e-02  3.19689989e+00  5.07049978e-01\n",
      "  -1.09840000e+00 -6.69990003e-01 -4.27170008e-01  3.33139986e-01\n",
      "  -5.24100006e-01  2.09270000e-01 -2.67309994e-01 -9.02930021e-01\n",
      "  -4.72389996e-01 -2.15179995e-01 -1.61080003e-01 -5.66910028e-01\n",
      "  -1.50499996e-02 -2.80930009e-02  1.96119994e-01  6.29209995e-01\n",
      "  -2.38749996e-01  1.72600001e-01]\n",
      " [-5.38190007e-01 -4.07880008e-01 -1.96300000e-01 -8.08210015e-01\n",
      "   7.71450028e-02  2.57849991e-02 -5.04819989e-01 -6.88870028e-02\n",
      "   1.36280000e-01  2.86190003e-01  8.21499974e-02 -2.68860012e-01\n",
      "   3.93500000e-01 -9.48450029e-01  2.16120005e-01 -1.94330007e-01\n",
      "   1.75880000e-01 -2.83730000e-01  5.34959972e-01 -2.61350006e-01\n",
      "  -5.45120001e-01  4.91430014e-01 -3.27520013e-01 -4.06620018e-02\n",
      "   4.16889995e-01 -1.02610004e+00 -5.67520000e-02 -4.31340009e-01\n",
      "   5.06919980e-01  6.98109984e-01  3.54509997e+00  7.90690005e-01\n",
      "   7.54549980e-01 -4.28250015e-01  7.56919980e-02  3.54660004e-01\n",
      "  -3.89970005e-01 -2.14450005e-02  5.58510005e-01 -5.66009998e-01\n",
      "  -7.96440005e-01  3.87879983e-02  7.94690013e-01  3.25060010e-01\n",
      "   2.09749997e-01  2.02480003e-01 -6.31810009e-01  5.21799996e-02\n",
      "   1.07309997e+00  3.56290005e-02]\n",
      " [ 2.76910007e-01  2.87449986e-01 -2.99349993e-01 -1.99640006e-01\n",
      "   1.29559994e-01  1.55550003e-01 -6.45219982e-01 -3.40900004e-01\n",
      "  -1.18330002e-01  1.57979995e-01  1.39689997e-01  2.48720005e-01\n",
      "  -1.59009993e-01 -3.34389992e-02  1.18950002e-01  7.65350014e-02\n",
      "   4.52630013e-01  2.64939994e-01 -1.91569999e-01 -5.67680001e-01\n",
      "   2.92860009e-02  2.17449993e-01  4.34060007e-01  1.49810001e-01\n",
      "   7.57739991e-02 -1.44529998e+00 -5.83940029e-01 -4.60629985e-02\n",
      "   6.62140027e-02 -2.64169991e-01  3.96499991e+00  2.51960009e-01\n",
      "   2.48549998e-01 -5.05240023e-01  2.58060008e-01  2.86830008e-01\n",
      "  -1.79940000e-01  6.28849983e-01 -1.20399997e-01 -4.21429984e-02\n",
      "  -4.49110009e-02  1.85609996e-01  1.62660003e-01 -2.61269999e-03\n",
      "   1.30830005e-01  2.01790005e-01 -2.96669990e-01 -9.48200002e-02\n",
      "  -2.12500006e-01  2.20740009e-02]\n",
      " [-2.65670009e-02  1.33570004e+00 -1.02800000e+00 -3.72900009e-01\n",
      "   5.20120025e-01 -1.26990005e-01 -3.54330003e-01  3.78239989e-01\n",
      "  -2.97160000e-01  9.38939974e-02 -3.41220014e-02  9.29610014e-01\n",
      "  -1.40230000e-01 -6.32990003e-01  2.08010003e-02 -2.15330005e-01\n",
      "   9.69229996e-01  4.76539999e-01 -1.00390005e+00 -2.40130007e-01\n",
      "  -3.63249987e-01 -4.75700013e-03 -5.14800012e-01 -4.62599993e-01\n",
      "   1.24469995e+00 -1.83159995e+00 -1.55809999e+00 -3.74650002e-01\n",
      "   5.33620000e-01  2.08829999e-01  3.22090006e+00  6.45489991e-01\n",
      "   3.74379992e-01 -1.76569998e-01 -2.41640005e-02  3.37859988e-01\n",
      "  -4.19000000e-01  4.00810003e-01 -1.14490002e-01  5.12319990e-02\n",
      "  -1.52050003e-01  2.98550010e-01 -4.40519989e-01  1.10890001e-01\n",
      "  -2.46329993e-01  6.62509978e-01 -2.69490004e-01 -4.96580005e-01\n",
      "  -4.16180015e-01 -2.54900008e-01]\n",
      " [ 2.01890007e-01  8.06060016e-01 -1.12810004e+00 -5.95929980e-01\n",
      "   5.27559996e-01 -4.76900011e-01 -5.26400030e-01  1.45260006e-01\n",
      "  -8.60870004e-01  5.61990023e-01 -4.37079996e-01 -1.65859997e-01\n",
      "  -2.33280003e-01 -2.17260003e-01  5.21139979e-01  6.23070002e-02\n",
      "   5.51150024e-01 -1.80020005e-01 -3.29829991e-01 -9.44339991e-01\n",
      "  -6.20190024e-01  7.87639976e-01 -3.61330003e-01  6.85800016e-01\n",
      "   3.79099995e-01 -8.77439976e-01 -7.67920017e-01  1.28849995e+00\n",
      "   1.14199996e+00 -7.34889984e-01  2.39319992e+00  1.09669995e+00\n",
      "  -4.86860007e-01  5.28400004e-01  3.92699987e-01 -5.64269982e-02\n",
      "   2.96319991e-01  1.07980001e+00  4.51570004e-01 -9.81149971e-01\n",
      "   1.00370002e+00  1.56340003e-01  2.25840006e-02 -1.48320004e-01\n",
      "  -9.29329991e-02  3.36910009e-01  4.21710014e-01 -2.16419995e-01\n",
      "   1.01390004e+00  1.05350006e+00]\n",
      " [-4.04309988e-01  7.80019999e-01 -6.75379992e-01 -9.71489996e-02\n",
      "   5.42420030e-01 -5.12830019e-01 -4.77580011e-01 -1.14289999e-01\n",
      "   5.51940024e-01  4.95299995e-01  2.56810009e-01  4.15160000e-01\n",
      "   3.82230014e-01  3.39360014e-02 -5.19810021e-01 -5.75200021e-01\n",
      "   5.09750009e-01  4.40290004e-01 -1.68850005e-01 -7.56869972e-01\n",
      "  -3.67469996e-01  3.72689992e-01 -8.66999999e-02 -3.10570002e-01\n",
      "   7.03400016e-01 -4.68490005e-01 -4.78450000e-01  1.57940000e-01\n",
      "   2.39690006e-01  1.61039993e-01  2.76320004e+00  3.33810002e-01\n",
      "   4.79530007e-01 -3.73100013e-01  7.10950017e-01  6.87900007e-01\n",
      "   6.73990026e-02  1.41770005e+00 -3.56299996e-01 -4.81590003e-01\n",
      "   5.32809973e-01 -7.45930001e-02  1.60280000e-02  2.33709998e-02\n",
      "  -2.95299999e-02 -1.24629997e-01  1.25009999e-01  7.22549975e-01\n",
      "   4.75899994e-01  7.95140028e-01]\n",
      " [-2.62410015e-01 -1.11029994e+00  5.02709985e-01 -4.30519998e-01\n",
      "   3.74680012e-01 -3.05500001e-01  3.67080003e-01  2.59380013e-01\n",
      "  -1.69929996e-01  5.42450011e-01  6.39190018e-01  1.13470003e-01\n",
      "  -3.91900003e-01  3.15210015e-01 -4.29010004e-01  4.99769986e-01\n",
      "  -2.37599999e-01 -7.93070018e-01  3.44940007e-01 -4.78769988e-01\n",
      "  -5.19450009e-01 -5.06649971e-01  5.77009991e-02 -3.17970008e-01\n",
      "  -8.01339969e-02 -1.02890003e+00 -1.50700003e-01  5.09440005e-01\n",
      "   6.07150018e-01  1.30490005e+00  3.25749993e+00  1.18490003e-01\n",
      "   1.50569999e+00 -3.66490006e-01 -1.77259997e-01 -2.09309995e-01\n",
      "  -5.95269978e-01 -2.58889999e-02 -2.96499997e-01 -1.13870001e+00\n",
      "  -5.29990017e-01  6.72859997e-02  9.49539989e-02  4.97220010e-02\n",
      "   5.13230026e-01 -1.11939996e-01 -7.11099990e-03  2.37749994e-01\n",
      "   6.88740015e-01  1.38730004e-01]\n",
      " [ 5.50249994e-01 -2.49420002e-01 -9.38599987e-04 -2.63999999e-01\n",
      "   5.93200028e-01  2.79500008e-01 -2.56660014e-01  9.30759981e-02\n",
      "  -3.62879992e-01  9.07759964e-02  2.84090012e-01  7.13370025e-01\n",
      "  -4.75100011e-01 -2.44130000e-01  8.84239972e-01  8.91089976e-01\n",
      "   4.30090010e-01 -2.73299992e-01  1.12760000e-01 -8.16649973e-01\n",
      "  -4.12719995e-01  1.77540004e-01  6.19419992e-01  1.04659997e-01\n",
      "   3.33270013e-01 -2.31250000e+00 -5.23710012e-01 -2.18979996e-02\n",
      "   5.38010001e-01 -5.06150007e-01  3.86829996e+00  1.66419998e-01\n",
      "  -7.19810009e-01 -7.47280002e-01  1.16310000e-01 -3.75849992e-01\n",
      "   5.55199981e-01  1.26750007e-01 -2.26420000e-01 -1.01750001e-01\n",
      "  -3.54550004e-01  1.23480000e-01  1.65319994e-01  7.04200029e-01\n",
      "  -8.02310035e-02 -6.84060007e-02 -6.76259995e-01  3.37630004e-01\n",
      "   5.01389988e-02  3.34650010e-01]\n",
      " [ 9.07540023e-01 -3.83219987e-01  6.76479995e-01 -2.02219993e-01\n",
      "   1.51559994e-01  1.36270002e-01 -4.88130003e-01  4.82230008e-01\n",
      "  -9.57150012e-02  1.83060005e-01  2.70069987e-01  4.14150000e-01\n",
      "  -4.89329994e-01 -7.60049978e-03  7.96620011e-01  1.09889996e+00\n",
      "   5.38020015e-01 -5.44679999e-01 -1.60630003e-01 -9.83479977e-01\n",
      "  -1.91880003e-01 -2.14399993e-01  1.99589998e-01 -3.13410014e-01\n",
      "   2.41009995e-01 -2.26620007e+00 -2.59259999e-01 -1.08980000e-01\n",
      "   6.61769986e-01 -4.81040001e-01  3.62980008e+00  4.53969985e-01\n",
      "  -6.44840002e-01 -5.22440016e-01  4.29220013e-02 -1.66050002e-01\n",
      "   9.71020013e-02  4.48359996e-02  2.03889996e-01 -4.63220000e-01\n",
      "  -4.64340001e-01  3.23940009e-01  2.59840012e-01  4.08490002e-01\n",
      "   2.03510001e-01  5.87220006e-02 -1.64079994e-01  2.06719995e-01\n",
      "  -1.84400007e-01  7.11470023e-02]\n",
      " [ 9.49109972e-01 -3.49680007e-01  4.81249988e-01 -1.93059996e-01\n",
      "  -8.83840024e-03  2.81819999e-01 -9.61300015e-01 -1.35810003e-01\n",
      "  -4.30830002e-01 -9.29329991e-02  1.56890005e-01  5.95850013e-02\n",
      "  -4.96349990e-01 -1.74140006e-01  7.56609976e-01  4.92100000e-01\n",
      "   2.17730001e-01 -2.27779999e-01 -1.36859998e-01 -9.05889988e-01\n",
      "  -4.87809986e-01  1.99190006e-01  9.14470017e-01 -1.62029997e-01\n",
      "  -2.06450000e-01 -1.73119998e+00 -4.76220012e-01 -4.85399999e-02\n",
      "  -1.40269995e-01 -4.58279997e-01  4.03259993e+00  6.05199993e-01\n",
      "   1.04479998e-01 -7.36100018e-01  2.48500004e-01 -3.34610008e-02\n",
      "  -1.33949995e-01  5.27819991e-02 -2.72680014e-01  7.98249990e-02\n",
      "  -8.01270008e-01  3.08310002e-01  4.35669988e-01  8.87470007e-01\n",
      "   2.98159987e-01 -2.46500000e-02 -9.50749993e-01  3.62329990e-01\n",
      "  -7.25120008e-01 -6.08900011e-01]\n",
      " [-1.20899998e-01 -1.68210000e-01  2.40989998e-01 -3.02870005e-01\n",
      "   4.35779989e-01 -3.83670002e-01 -5.52030027e-01 -2.86810011e-01\n",
      "  -1.00919999e-01  4.77690011e-01  2.89689988e-01  2.95489997e-01\n",
      "  -4.40739989e-01 -1.34939998e-01  2.60219991e-01  4.53709990e-01\n",
      "   5.37490010e-01  6.12200014e-02  2.43660003e-01 -8.77610028e-01\n",
      "  -5.62409997e-01  2.49270007e-01  1.79409996e-01 -1.69430003e-02\n",
      "   5.79739988e-01 -1.35459995e+00 -5.31610012e-01 -2.64510006e-01\n",
      "   6.82110012e-01 -3.04820001e-01  3.79430008e+00  9.63259995e-01\n",
      "  -7.38959983e-02 -4.10320014e-01  2.44780004e-01 -1.45789996e-01\n",
      "  -8.67889971e-02  9.95000005e-01  4.99279983e-02 -6.03020012e-01\n",
      "  -3.65850002e-01 -1.01140000e-01  4.04229999e-01  2.59510010e-01\n",
      "   8.79269987e-02  6.19600005e-02  7.52660036e-02  1.27550006e-01\n",
      "   6.64609969e-02  1.11629999e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(len(embedding_matrix))\n",
    "print(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = Embedding(vocab_size, 50, weights=[embedding_matrix], input_length=max_length, trainable=False)\n",
    "# max_length = 4 -> maximal 4 Wörter in einem Sentiment text\n",
    "# der Output muss immer die gleiche Anzahl haben, wie Gewichte Dimension\n",
    "# trainable=False -> wir wollen die gelernten Gewichte nicht verändern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 4, 50)             750       \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 951\n",
      "Trainable params: 201\n",
      "Non-trainable params: 750\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89.999998\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
